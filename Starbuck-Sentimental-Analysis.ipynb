{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Presented By: Anusha Rajan & Sandhya Kumari"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 393,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing tweepy package\n",
    "import tweepy\n",
    "from tweepy import OAuthHandler\n",
    "import numpy as np\n",
    "# Importing pandas package\n",
    "import pandas as pd\n",
    "pd.set_option('display.max_colwidth', 500) #in order to get all text from tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 394,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'>\n",
      "500\n"
     ]
    }
   ],
   "source": [
    "consumer_key =  \"\"\n",
    "consumer_secret = \"\"\n",
    "access_token= \"\"\n",
    "access_secret=\"\"\n",
    "\n",
    "auth = OAuthHandler(consumer_key, consumer_secret)\n",
    "auth.set_access_token(access_token, access_secret)\n",
    "\n",
    "api = tweepy.API(auth)\n",
    "\n",
    "results = []\n",
    "\n",
    "#Gathering 500 tweets regarding shutdownimpact \n",
    "for tweet in tweepy.Cursor(api.search, q='%23starbucks').items(500):\n",
    "    results.append(tweet)\n",
    "\n",
    "#Verify items being returned\n",
    "print(type(results))\n",
    "print(len(results))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 395,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Defining Data Frame\n",
    "def toDataFrame(tweets):\n",
    "\n",
    "    DataSet = pd.DataFrame()\n",
    "\n",
    "    DataSet['tweetText'] = [tweet.text for tweet in tweets]\n",
    "    DataSet['tweetRetweetCt'] = [tweet.retweet_count for tweet \n",
    "    in tweets]\n",
    "    DataSet['tweetFavoriteCt'] = [tweet.favorite_count for tweet \n",
    "    in tweets]\n",
    "    DataSet['tweetSource'] = [tweet.source for tweet in tweets]\n",
    "    DataSet['tweetCreated'] = [tweet.created_at for tweet in tweets]\n",
    "    DataSet['userScreen'] = [tweet.user.screen_name for tweet \n",
    "    in tweets]\n",
    "    DataSet['userName'] = [tweet.user.name for tweet in tweets]\n",
    "    DataSet['userCreateDt'] = [tweet.user.created_at for tweet \n",
    "    in tweets]\n",
    "    DataSet['userDesc'] = [tweet.user.description for tweet in tweets]\n",
    "    DataSet['userFollowerCt'] = [tweet.user.followers_count for tweet \n",
    "    in tweets]\n",
    "    DataSet['userFriendsCt'] = [tweet.user.friends_count for tweet \n",
    "    in tweets]\n",
    "    DataSet['userLocation'] = [tweet.user.location for tweet in tweets]\n",
    "\n",
    "    return DataSet\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 396,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(500, 12)\n"
     ]
    }
   ],
   "source": [
    "#Putting tweets into data frame\n",
    "tweet_frame = toDataFrame(results)\n",
    "print(tweet_frame.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 397,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweetText</th>\n",
       "      <th>tweetRetweetCt</th>\n",
       "      <th>tweetFavoriteCt</th>\n",
       "      <th>tweetSource</th>\n",
       "      <th>tweetCreated</th>\n",
       "      <th>userScreen</th>\n",
       "      <th>userName</th>\n",
       "      <th>userCreateDt</th>\n",
       "      <th>userDesc</th>\n",
       "      <th>userFollowerCt</th>\n",
       "      <th>userFriendsCt</th>\n",
       "      <th>userLocation</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>RT @PreorderInkorea: ‡πÅ‡∏Å‡πâ‡∏ß‡∏™‡∏ï‡∏≤‡∏£‡πå‡∏ö‡∏±‡∏Ñ‡πÄ‡∏Å‡∏≤‡∏´‡∏•‡∏µ ‡∏Ñ‡∏≠‡∏•‡∏ß‡∏≤‡πÄ‡∏•‡∏ô‡πÑ‡∏ó‡∏ô‡πå\\n‡∏™‡∏µ‡∏ä‡∏°‡∏û‡∏π ‡∏´‡∏±‡∏ß‡πÉ‡∏à ‡∏´‡∏ß‡∏≤‡∏ô‡πÜ‡πÜ\\n\\n#‡∏™‡∏ï‡∏≤‡∏£‡πå‡∏ö‡∏±‡∏Ñ‡∏™‡πå #‡πÅ‡∏Å‡πâ‡∏ß‡∏™‡∏ï‡∏≤‡∏£‡πå‡∏ö‡∏±‡∏Ñ #starbucks https://t.co/9CmGirSQaL</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>Twitter for iPhone</td>\n",
       "      <td>2019-02-02 03:12:05</td>\n",
       "      <td>pang_pang332</td>\n",
       "      <td>shine on meüå•</td>\n",
       "      <td>2013-01-27 00:36:17</td>\n",
       "      <td></td>\n",
       "      <td>7</td>\n",
       "      <td>78</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Break time „Ç§„Ç™„É≥„É¢„Éº„É´È´òÁü•Â∫ó #starbucks https://t.co/W0Xo7zIDb8</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>Twitter for iPhone</td>\n",
       "      <td>2019-02-02 03:08:52</td>\n",
       "      <td>ShojiYamamoto86</td>\n",
       "      <td>Â±± Êú¨  Â∞Ü Âè∏</td>\n",
       "      <td>2015-10-19 09:33:30</td>\n",
       "      <td>Cyclistüòé Kochi Technical High School. Next...Kyoto Sangyo University.</td>\n",
       "      <td>1520</td>\n",
       "      <td>956</td>\n",
       "      <td>Kochi JAPAN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>RT @GolfWriterKiel: Fun coffee fact: The #Starbucks brand is built on a lie. The lie is that coffee that tastes \"strong\" has lots of caffei‚Ä¶</td>\n",
       "      <td>3202</td>\n",
       "      <td>0</td>\n",
       "      <td>Twitter for iPhone</td>\n",
       "      <td>2019-02-02 03:07:52</td>\n",
       "      <td>WillNemitz</td>\n",
       "      <td>William Nemitz</td>\n",
       "      <td>2012-07-31 15:09:00</td>\n",
       "      <td></td>\n",
       "      <td>306</td>\n",
       "      <td>4731</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>RT @GolfWriterKiel: Fun coffee fact: The #Starbucks brand is built on a lie. The lie is that coffee that tastes \"strong\" has lots of caffei‚Ä¶</td>\n",
       "      <td>3202</td>\n",
       "      <td>0</td>\n",
       "      <td>Twitter Web Client</td>\n",
       "      <td>2019-02-02 03:06:14</td>\n",
       "      <td>MissAbsinthe</td>\n",
       "      <td>Cassandra Tells You So</td>\n",
       "      <td>2009-07-30 06:48:22</td>\n",
       "      <td>Making it up as I go...</td>\n",
       "      <td>4305</td>\n",
       "      <td>5001</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>RT @GolfWriterKiel: Fun coffee fact: The #Starbucks brand is built on a lie. The lie is that coffee that tastes \"strong\" has lots of caffei‚Ä¶</td>\n",
       "      <td>3202</td>\n",
       "      <td>0</td>\n",
       "      <td>Twitter Web Client</td>\n",
       "      <td>2019-02-02 03:05:57</td>\n",
       "      <td>anonsgreetings</td>\n",
       "      <td>miscat@RetweetCity</td>\n",
       "      <td>2014-07-26 19:34:53</td>\n",
       "      <td>...Hi, I like memes and fandom stuff. This account may have anime/game spoilers/nsfw. I tend to forget about social media for months on end.\\nBlog:active</td>\n",
       "      <td>18</td>\n",
       "      <td>90</td>\n",
       "      <td>Twitter</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                      tweetText  \\\n",
       "0     RT @PreorderInkorea: ‡πÅ‡∏Å‡πâ‡∏ß‡∏™‡∏ï‡∏≤‡∏£‡πå‡∏ö‡∏±‡∏Ñ‡πÄ‡∏Å‡∏≤‡∏´‡∏•‡∏µ ‡∏Ñ‡∏≠‡∏•‡∏ß‡∏≤‡πÄ‡∏•‡∏ô‡πÑ‡∏ó‡∏ô‡πå\\n‡∏™‡∏µ‡∏ä‡∏°‡∏û‡∏π ‡∏´‡∏±‡∏ß‡πÉ‡∏à ‡∏´‡∏ß‡∏≤‡∏ô‡πÜ‡πÜ\\n\\n#‡∏™‡∏ï‡∏≤‡∏£‡πå‡∏ö‡∏±‡∏Ñ‡∏™‡πå #‡πÅ‡∏Å‡πâ‡∏ß‡∏™‡∏ï‡∏≤‡∏£‡πå‡∏ö‡∏±‡∏Ñ #starbucks https://t.co/9CmGirSQaL   \n",
       "1                                                                                       Break time „Ç§„Ç™„É≥„É¢„Éº„É´È´òÁü•Â∫ó #starbucks https://t.co/W0Xo7zIDb8   \n",
       "2  RT @GolfWriterKiel: Fun coffee fact: The #Starbucks brand is built on a lie. The lie is that coffee that tastes \"strong\" has lots of caffei‚Ä¶   \n",
       "3  RT @GolfWriterKiel: Fun coffee fact: The #Starbucks brand is built on a lie. The lie is that coffee that tastes \"strong\" has lots of caffei‚Ä¶   \n",
       "4  RT @GolfWriterKiel: Fun coffee fact: The #Starbucks brand is built on a lie. The lie is that coffee that tastes \"strong\" has lots of caffei‚Ä¶   \n",
       "\n",
       "   tweetRetweetCt  tweetFavoriteCt         tweetSource        tweetCreated  \\\n",
       "0               6                0  Twitter for iPhone 2019-02-02 03:12:05   \n",
       "1               0                1  Twitter for iPhone 2019-02-02 03:08:52   \n",
       "2            3202                0  Twitter for iPhone 2019-02-02 03:07:52   \n",
       "3            3202                0  Twitter Web Client 2019-02-02 03:06:14   \n",
       "4            3202                0  Twitter Web Client 2019-02-02 03:05:57   \n",
       "\n",
       "        userScreen                userName        userCreateDt  \\\n",
       "0     pang_pang332            shine on meüå• 2013-01-27 00:36:17   \n",
       "1  ShojiYamamoto86                Â±± Êú¨  Â∞Ü Âè∏ 2015-10-19 09:33:30   \n",
       "2       WillNemitz          William Nemitz 2012-07-31 15:09:00   \n",
       "3     MissAbsinthe  Cassandra Tells You So 2009-07-30 06:48:22   \n",
       "4   anonsgreetings      miscat@RetweetCity 2014-07-26 19:34:53   \n",
       "\n",
       "                                                                                                                                                    userDesc  \\\n",
       "0                                                                                                                                                              \n",
       "1                                                                                      Cyclistüòé Kochi Technical High School. Next...Kyoto Sangyo University.   \n",
       "2                                                                                                                                                              \n",
       "3                                                                                                                                    Making it up as I go...   \n",
       "4  ...Hi, I like memes and fandom stuff. This account may have anime/game spoilers/nsfw. I tend to forget about social media for months on end.\\nBlog:active   \n",
       "\n",
       "   userFollowerCt  userFriendsCt userLocation  \n",
       "0               7             78               \n",
       "1            1520            956  Kochi JAPAN  \n",
       "2             306           4731               \n",
       "3            4305           5001               \n",
       "4              18             90      Twitter  "
      ]
     },
     "execution_count": 397,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Show head of tweet data frame\n",
    "tweet_frame.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 398,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweetText</th>\n",
       "      <th>tweetRetweetCt</th>\n",
       "      <th>tweetFavoriteCt</th>\n",
       "      <th>tweetSource</th>\n",
       "      <th>tweetCreated</th>\n",
       "      <th>userScreen</th>\n",
       "      <th>userName</th>\n",
       "      <th>userCreateDt</th>\n",
       "      <th>userDesc</th>\n",
       "      <th>userFollowerCt</th>\n",
       "      <th>userFriendsCt</th>\n",
       "      <th>userLocation</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>495</th>\n",
       "      <td>RT @GolfWriterKiel: Fun coffee fact: The #Starbucks brand is built on a lie. The lie is that coffee that tastes \"strong\" has lots of caffei‚Ä¶</td>\n",
       "      <td>3202</td>\n",
       "      <td>0</td>\n",
       "      <td>Twitter for iPhone</td>\n",
       "      <td>2019-02-01 22:44:02</td>\n",
       "      <td>PoliticalMsging</td>\n",
       "      <td>Daniela Busciglio</td>\n",
       "      <td>2009-11-28 22:52:52</td>\n",
       "      <td>Professor. Applied Linguist. Political Operative. Comms &amp; Messaging Strategist/Consultant. Rescue Ranger. NJ Native. Yogi. The personal is always political.</td>\n",
       "      <td>686</td>\n",
       "      <td>3830</td>\n",
       "      <td>New Jersey, USA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>496</th>\n",
       "      <td>RT @MariaGrasmick: üçê PRESIDENT TRUMP !  ! üçê A Political Cartoon for you. EX #starbucks Former Starbucks CEO Howard Schultz Presidential Bid‚Ä¶</td>\n",
       "      <td>54</td>\n",
       "      <td>0</td>\n",
       "      <td>Twitter for iPhone</td>\n",
       "      <td>2019-02-01 22:43:24</td>\n",
       "      <td>CharlieCanFly</td>\n",
       "      <td>Mister Flowers</td>\n",
       "      <td>2017-07-14 23:26:05</td>\n",
       "      <td>if you personally stop ‚úã being evil it will help everyone. do you understand</td>\n",
       "      <td>2504</td>\n",
       "      <td>2787</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497</th>\n",
       "      <td>RT @MariaGrasmick: üçê PRESIDENT TRUMP !  ! üçê A Political Cartoon for you. EX #starbucks Former Starbucks CEO Howard Schultz Presidential Bid‚Ä¶</td>\n",
       "      <td>54</td>\n",
       "      <td>0</td>\n",
       "      <td>Twitter for Android</td>\n",
       "      <td>2019-02-01 22:42:45</td>\n",
       "      <td>alley167</td>\n",
       "      <td>Americanüá∫üá∏Alley I am Flynn‚≠ê‚≠ê‚≠ê</td>\n",
       "      <td>2011-06-20 22:44:04</td>\n",
       "      <td>PRO-TRUMP, #2A Love God &amp; support those in Uniform üá∫üá∏üá∫üá∏üá∫üá∏üá∫üá∏üá∫üá∏üá∫üá∏üá∫üá∏üá∫üá∏\\nMarried ‚ùéNO LIST üö´No DM!\\nüî∫Ô∏èVOTE RED TO SAVE AMERICA üîª</td>\n",
       "      <td>35477</td>\n",
       "      <td>35882</td>\n",
       "      <td>Ohio, USA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498</th>\n",
       "      <td>RT @GolfWriterKiel: Fun coffee fact: The #Starbucks brand is built on a lie. The lie is that coffee that tastes \"strong\" has lots of caffei‚Ä¶</td>\n",
       "      <td>3202</td>\n",
       "      <td>0</td>\n",
       "      <td>Twitter for Android</td>\n",
       "      <td>2019-02-01 22:42:37</td>\n",
       "      <td>ramoth1999</td>\n",
       "      <td>Brenda Montgomery</td>\n",
       "      <td>2011-01-17 16:32:38</td>\n",
       "      <td>#theresistance  üåäüåä Democratic socialist hate Dotard and most of the GOP. Hate fascists.Gun laws needed. in relationship  no MAGAts please #resistance</td>\n",
       "      <td>2397</td>\n",
       "      <td>3500</td>\n",
       "      <td>Crestview, FL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>499</th>\n",
       "      <td>RT @GolfWriterKiel: Fun coffee fact: The #Starbucks brand is built on a lie. The lie is that coffee that tastes \"strong\" has lots of caffei‚Ä¶</td>\n",
       "      <td>3202</td>\n",
       "      <td>0</td>\n",
       "      <td>Twitter for iPhone</td>\n",
       "      <td>2019-02-01 22:42:15</td>\n",
       "      <td>Gundisalvus1</td>\n",
       "      <td>Gonzo</td>\n",
       "      <td>2011-10-27 19:52:07</td>\n",
       "      <td></td>\n",
       "      <td>71</td>\n",
       "      <td>185</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                        tweetText  \\\n",
       "495  RT @GolfWriterKiel: Fun coffee fact: The #Starbucks brand is built on a lie. The lie is that coffee that tastes \"strong\" has lots of caffei‚Ä¶   \n",
       "496  RT @MariaGrasmick: üçê PRESIDENT TRUMP !  ! üçê A Political Cartoon for you. EX #starbucks Former Starbucks CEO Howard Schultz Presidential Bid‚Ä¶   \n",
       "497  RT @MariaGrasmick: üçê PRESIDENT TRUMP !  ! üçê A Political Cartoon for you. EX #starbucks Former Starbucks CEO Howard Schultz Presidential Bid‚Ä¶   \n",
       "498  RT @GolfWriterKiel: Fun coffee fact: The #Starbucks brand is built on a lie. The lie is that coffee that tastes \"strong\" has lots of caffei‚Ä¶   \n",
       "499  RT @GolfWriterKiel: Fun coffee fact: The #Starbucks brand is built on a lie. The lie is that coffee that tastes \"strong\" has lots of caffei‚Ä¶   \n",
       "\n",
       "     tweetRetweetCt  tweetFavoriteCt          tweetSource        tweetCreated  \\\n",
       "495            3202                0   Twitter for iPhone 2019-02-01 22:44:02   \n",
       "496              54                0   Twitter for iPhone 2019-02-01 22:43:24   \n",
       "497              54                0  Twitter for Android 2019-02-01 22:42:45   \n",
       "498            3202                0  Twitter for Android 2019-02-01 22:42:37   \n",
       "499            3202                0   Twitter for iPhone 2019-02-01 22:42:15   \n",
       "\n",
       "          userScreen                       userName        userCreateDt  \\\n",
       "495  PoliticalMsging              Daniela Busciglio 2009-11-28 22:52:52   \n",
       "496    CharlieCanFly                 Mister Flowers 2017-07-14 23:26:05   \n",
       "497         alley167  Americanüá∫üá∏Alley I am Flynn‚≠ê‚≠ê‚≠ê 2011-06-20 22:44:04   \n",
       "498       ramoth1999              Brenda Montgomery 2011-01-17 16:32:38   \n",
       "499     Gundisalvus1                          Gonzo 2011-10-27 19:52:07   \n",
       "\n",
       "                                                                                                                                                         userDesc  \\\n",
       "495  Professor. Applied Linguist. Political Operative. Comms & Messaging Strategist/Consultant. Rescue Ranger. NJ Native. Yogi. The personal is always political.   \n",
       "496                                                                                  if you personally stop ‚úã being evil it will help everyone. do you understand   \n",
       "497                                   PRO-TRUMP, #2A Love God & support those in Uniform üá∫üá∏üá∫üá∏üá∫üá∏üá∫üá∏üá∫üá∏üá∫üá∏üá∫üá∏üá∫üá∏\\nMarried ‚ùéNO LIST üö´No DM!\\nüî∫Ô∏èVOTE RED TO SAVE AMERICA üîª   \n",
       "498         #theresistance  üåäüåä Democratic socialist hate Dotard and most of the GOP. Hate fascists.Gun laws needed. in relationship  no MAGAts please #resistance   \n",
       "499                                                                                                                                                                 \n",
       "\n",
       "     userFollowerCt  userFriendsCt     userLocation  \n",
       "495             686           3830  New Jersey, USA  \n",
       "496            2504           2787                   \n",
       "497           35477          35882        Ohio, USA  \n",
       "498            2397           3500    Crestview, FL  \n",
       "499              71            185                   "
      ]
     },
     "execution_count": 398,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Show tail of tweet data frame\n",
    "tweet_frame.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q1: For our data frame, We decided to pull tweets featuring '#Starbucks', since our company helps cafe and restaurants analyze their data which helps their business serve better.Starbucks being a successful coffee chain takes their customer satisfaction very seriously, not to mention it is one of their core values. So, Starbucks recently gave us the project to analyze their data based on which they can understand their customers of different age demographics from different parts of the world better. Using just the tweet text, we can pull information about what people are associated with starbucks for positive or negative feedbacks as well as tweets show what the coffee giant is doing for the people. We can answer questions such as Are people in general concerned with the prices, their coffee, the working staff, customer satisfaction, starbucks has more to it than just coffee and more. Utilizing this information, we can possibly get a feel of how people around the world feel about their coffee or concerns about the starbucks firm. Also, posts/advertise how starbucks are helping people around the world. For example the tweet about helping people aged 52-66 work.\n",
    "\n",
    "Besides the tweet itself, we can use information, such as user location, to perhaps know where certain tweets are coming from to get a better idea of what certain parts of the country or globally are thinking, or if people outside the country have thoughts of Starbucks. We can also use the user description to attempt to see if certain professions (songwriter), since people sometimes list their profession there, have particular feelings or thoughts regarding Starbucks, perhaps what age or profession the tweets are coming from can effect in many ways. Another piece of information that could be used is follower count, to see whose message is getting out to more people."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task:2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 399,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import math\n",
    "#print(tweet_frame['tweetText'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 400,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'scipy.sparse.csr.csr_matrix'>\n",
      "(500, 1220)\n"
     ]
    }
   ],
   "source": [
    " # version 1 : \n",
    "    \n",
    " #defines the method by which we will turn the list of words into a numeric representation\n",
    "\n",
    "cv1 = CountVectorizer(binary=True)\n",
    "\n",
    "cv1_tweets = cv1.fit_transform(tweet_frame['tweetText'])\n",
    "\n",
    "print(type(cv1_tweets))\n",
    "print(cv1_tweets.shape)\n",
    "#print(cv0_tweets)\n",
    "#what do these numbers mean?\n",
    "#cv1.get_feature_names() # by default lowercase characters will come, you've do differently to get in capital\n",
    "names = cv1.get_feature_names()\n",
    "count = np.sum(cv1_tweets.toarray(), axis = 0) # add up feature counts \n",
    "count2 = count.tolist()  # convert numpy array to list\n",
    "count_df = pd.DataFrame(count2, index = names, columns = ['count'])\n",
    "#count_df.sort_values(['count'], ascending = False)  #arrange by count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 401,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'scipy.sparse.csr.csr_matrix'>\n",
      "(500, 2930)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "  # version 2 : \n",
    "cv2 = CountVectorizer(binary=False, min_df=.001,max_df= .005,ngram_range = (1,2), lowercase = False)\n",
    "\n",
    "#cv2\n",
    "# Apply that transformation to our text\n",
    "cv2_tweets = cv2.fit_transform(tweet_frame['tweetText'])\n",
    "\n",
    "print(type(cv2_tweets))\n",
    "print(cv2_tweets.shape)\n",
    "#print(cv2_tweets)\n",
    "#cv2.get_feature_names() \n",
    "\n",
    "names = cv2.get_feature_names()\n",
    "count = np.sum(cv2_tweets.toarray(), axis = 0) # add up feature counts \n",
    "count2 = count.tolist()  # convert numpy array to list\n",
    "count_df = pd.DataFrame(count2, index = names, columns = ['count'])\n",
    "#count_df.sort_values(['count'], ascending = False)  #arrange by count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 402,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(500, 69)\n"
     ]
    }
   ],
   "source": [
    "# version 3: \n",
    "\n",
    "cv3 = CountVectorizer(binary=False, min_df=.005, max_df= .009,ngram_range = (1,2), stop_words = \"english\") \n",
    "\n",
    "#apply the transformation\n",
    "cv3_tweets = cv3.fit_transform(tweet_frame['tweetText'])\n",
    "print(cv3_tweets.shape)\n",
    "\n",
    "# \"bag of words\"\n",
    "names = cv3.get_feature_names()\n",
    "count = np.sum(cv3_tweets.toarray(), axis = 0) # add up feature counts \n",
    "count2 = count.tolist()  # convert numpy array to list\n",
    "count_df = pd.DataFrame(count2, index = names, columns = ['count'])\n",
    "#count_df.sort_values(['count'], ascending = False)  #arrange by count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 403,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(500, 69)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>2019</th>\n",
       "      <th>25</th>\n",
       "      <th>25 starbucks</th>\n",
       "      <th>amazon</th>\n",
       "      <th>bene</th>\n",
       "      <th>beverage insulator</th>\n",
       "      <th>breitbartnews</th>\n",
       "      <th>check starbucks</th>\n",
       "      <th>coffee https</th>\n",
       "      <th>coffee mug</th>\n",
       "      <th>...</th>\n",
       "      <th>rt robbytreeroot</th>\n",
       "      <th>share</th>\n",
       "      <th>split</th>\n",
       "      <th>style</th>\n",
       "      <th>tail</th>\n",
       "      <th>thanks</th>\n",
       "      <th>time</th>\n",
       "      <th>today</th>\n",
       "      <th>treat</th>\n",
       "      <th>years</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.830312</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10 rows √ó 69 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   2019   25  25 starbucks  amazon  bene  beverage insulator  breitbartnews  \\\n",
       "0   0.0  0.0           0.0     0.0   0.0                 0.0            0.0   \n",
       "1   0.0  0.0           0.0     0.0   0.0                 0.0            0.0   \n",
       "2   0.0  0.0           0.0     0.0   0.0                 0.0            0.0   \n",
       "3   0.0  0.0           0.0     0.0   0.0                 0.0            0.0   \n",
       "4   0.0  0.0           0.0     0.0   0.0                 0.0            0.0   \n",
       "5   0.0  0.0           0.0     0.0   0.0                 0.0            0.0   \n",
       "6   0.0  0.0           0.0     0.0   0.0                 0.0            0.0   \n",
       "7   0.0  0.0           0.0     0.0   0.0                 0.0            0.0   \n",
       "8   0.0  0.0           0.0     0.0   0.0                 0.0            0.0   \n",
       "9   0.0  0.0           0.0     0.0   0.0                 0.0            0.0   \n",
       "\n",
       "   check starbucks  coffee https  coffee mug  ...    rt robbytreeroot  share  \\\n",
       "0              0.0           0.0         0.0  ...                 0.0    0.0   \n",
       "1              0.0           0.0         0.0  ...                 0.0    0.0   \n",
       "2              0.0           0.0         0.0  ...                 0.0    0.0   \n",
       "3              0.0           0.0         0.0  ...                 0.0    0.0   \n",
       "4              0.0           0.0         0.0  ...                 0.0    0.0   \n",
       "5              0.0           0.0         0.0  ...                 0.0    0.0   \n",
       "6              0.0           0.0         0.0  ...                 0.0    0.0   \n",
       "7              0.0           0.0         0.0  ...                 0.0    0.0   \n",
       "8              0.0           0.0         0.0  ...                 0.0    0.0   \n",
       "9              0.0           0.0         0.0  ...                 0.0    0.0   \n",
       "\n",
       "   split  style  tail  thanks      time  today  treat  years  \n",
       "0    0.0    0.0   0.0     0.0  0.000000    0.0    0.0    0.0  \n",
       "1    0.0    0.0   0.0     0.0  5.830312    0.0    0.0    0.0  \n",
       "2    0.0    0.0   0.0     0.0  0.000000    0.0    0.0    0.0  \n",
       "3    0.0    0.0   0.0     0.0  0.000000    0.0    0.0    0.0  \n",
       "4    0.0    0.0   0.0     0.0  0.000000    0.0    0.0    0.0  \n",
       "5    0.0    0.0   0.0     0.0  0.000000    0.0    0.0    0.0  \n",
       "6    0.0    0.0   0.0     0.0  0.000000    0.0    0.0    0.0  \n",
       "7    0.0    0.0   0.0     0.0  0.000000    0.0    0.0    0.0  \n",
       "8    0.0    0.0   0.0     0.0  0.000000    0.0    0.0    0.0  \n",
       "9    0.0    0.0   0.0     0.0  0.000000    0.0    0.0    0.0  \n",
       "\n",
       "[10 rows x 69 columns]"
      ]
     },
     "execution_count": 403,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#define the transformation with application of weights\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# weights based on the inverse document frequency\n",
    "tfidf3 = TfidfVectorizer(use_idf=True, norm=None, min_df=.005,max_df= .009,ngram_range = (1,2),stop_words = \"english\" ) \n",
    "\n",
    "#apply the transformation on our retrieved tweets\n",
    "tf3_tweets = tfidf3.fit_transform(tweet_frame['tweetText']) \n",
    "print(tf3_tweets.shape)\n",
    "\n",
    "pd.DataFrame(tf3_tweets.toarray(),columns = tfidf3.get_feature_names()).head(10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 419,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(500, 67)\n"
     ]
    }
   ],
   "source": [
    "# version 4: \n",
    "\n",
    " #define the transformation\n",
    "cv4 = CountVectorizer(binary=False, min_df=.009,max_df= .015,ngram_range = (1,2), stop_words = \"english\")\n",
    "cv4_tweets = cv4.fit_transform(tweet_frame['tweetText']) \n",
    "print(cv4_tweets.shape)\n",
    "\n",
    "# names of features of numbers in the matrix\n",
    "names = cv4.get_feature_names()\n",
    "# add up feature counts\n",
    "count = np.sum(cv4_tweets.toarray(), axis = 0)  \n",
    "count2 = count.tolist()  # convert numpy array to list\n",
    "count_df = pd.DataFrame(count2, index = names, columns = ['count'])\n",
    "#count_df.sort_values(['count'], ascending = False)  #arrange by count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 418,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(500, 67)\n"
     ]
    }
   ],
   "source": [
    "#define the transformation with application of weights\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "tfidf4 =TfidfVectorizer(use_idf=True, norm=None,binary=False, min_df=.009,max_df= .015,ngram_range = (1,2), stop_words=\"english\")\n",
    "\n",
    "#apply the transformation on our retrieved tweets\n",
    "tf4_tweets = tfidf4.fit_transform(tweet_frame['tweetText']) \n",
    "print(tf4_tweets.shape)\n",
    "pd.DataFrame(tf4_tweets.toarray(),columns = tfidf4.get_feature_names())\n",
    "\n",
    "names = tfidf4.get_feature_names()\n",
    "\n",
    "#count = np.max(tf2_tweets.toarray(), axis = 0) # find the max weight of each feature \n",
    "count = np.sum(tf4_tweets.toarray(), axis = 0) # Weighted feature space \n",
    "# convert numpy array to list\n",
    "count2 = count.tolist()  # convert numpy array to list\n",
    " # create a dataframe from the list\n",
    "count_df = pd.DataFrame(count2, index = names, columns = ['count']) # create a dataframe from the list\n",
    "sorted_count = count_df.sort_values(['count'], ascending = False)  #arrange by count instead\n",
    "#print(sorted_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 413,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>000</th>\n",
       "      <th>000 followers</th>\n",
       "      <th>37</th>\n",
       "      <th>37 000</th>\n",
       "      <th>amp</th>\n",
       "      <th>asp</th>\n",
       "      <th>beverage</th>\n",
       "      <th>boy</th>\n",
       "      <th>boycottstarbucks</th>\n",
       "      <th>boycottstarbucks howardschultz</th>\n",
       "      <th>...</th>\n",
       "      <th>starbucks home</th>\n",
       "      <th>starbucks shared</th>\n",
       "      <th>tested</th>\n",
       "      <th>tested failing</th>\n",
       "      <th>town</th>\n",
       "      <th>town schultz</th>\n",
       "      <th>vanity</th>\n",
       "      <th>vanity campaign</th>\n",
       "      <th>ve</th>\n",
       "      <th>„Çπ„Çø„Éº„Éê„ÉÉ„ÇØ„Çπ</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10 rows √ó 67 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   000  000 followers   37  37 000  amp  asp  beverage  boy  boycottstarbucks  \\\n",
       "0  0.0            0.0  0.0     0.0  0.0  0.0       0.0  0.0               0.0   \n",
       "1  0.0            0.0  0.0     0.0  0.0  0.0       0.0  0.0               0.0   \n",
       "2  0.0            0.0  0.0     0.0  0.0  0.0       0.0  0.0               0.0   \n",
       "3  0.0            0.0  0.0     0.0  0.0  0.0       0.0  0.0               0.0   \n",
       "4  0.0            0.0  0.0     0.0  0.0  0.0       0.0  0.0               0.0   \n",
       "5  0.0            0.0  0.0     0.0  0.0  0.0       0.0  0.0               0.0   \n",
       "6  0.0            0.0  0.0     0.0  0.0  0.0       0.0  0.0               0.0   \n",
       "7  0.0            0.0  0.0     0.0  0.0  0.0       0.0  0.0               0.0   \n",
       "8  0.0            0.0  0.0     0.0  0.0  0.0       0.0  0.0               0.0   \n",
       "9  0.0            0.0  0.0     0.0  0.0  0.0       0.0  0.0               0.0   \n",
       "\n",
       "   boycottstarbucks howardschultz   ...     starbucks home  starbucks shared  \\\n",
       "0                             0.0   ...                0.0               0.0   \n",
       "1                             0.0   ...                0.0               0.0   \n",
       "2                             0.0   ...                0.0               0.0   \n",
       "3                             0.0   ...                0.0               0.0   \n",
       "4                             0.0   ...                0.0               0.0   \n",
       "5                             0.0   ...                0.0               0.0   \n",
       "6                             0.0   ...                0.0               0.0   \n",
       "7                             0.0   ...                0.0               0.0   \n",
       "8                             0.0   ...                0.0               0.0   \n",
       "9                             0.0   ...                0.0               0.0   \n",
       "\n",
       "   tested  tested failing  town  town schultz  vanity  vanity campaign   ve  \\\n",
       "0     0.0             0.0   0.0           0.0     0.0              0.0  0.0   \n",
       "1     0.0             0.0   0.0           0.0     0.0              0.0  0.0   \n",
       "2     0.0             0.0   0.0           0.0     0.0              0.0  0.0   \n",
       "3     0.0             0.0   0.0           0.0     0.0              0.0  0.0   \n",
       "4     0.0             0.0   0.0           0.0     0.0              0.0  0.0   \n",
       "5     0.0             0.0   0.0           0.0     0.0              0.0  0.0   \n",
       "6     0.0             0.0   0.0           0.0     0.0              0.0  0.0   \n",
       "7     0.0             0.0   0.0           0.0     0.0              0.0  0.0   \n",
       "8     0.0             0.0   0.0           0.0     0.0              0.0  0.0   \n",
       "9     0.0             0.0   0.0           0.0     0.0              0.0  0.0   \n",
       "\n",
       "   „Çπ„Çø„Éº„Éê„ÉÉ„ÇØ„Çπ  \n",
       "0      0.0  \n",
       "1      0.0  \n",
       "2      0.0  \n",
       "3      0.0  \n",
       "4      0.0  \n",
       "5      0.0  \n",
       "6      0.0  \n",
       "7      0.0  \n",
       "8      0.0  \n",
       "9      0.0  \n",
       "\n",
       "[10 rows x 67 columns]"
      ]
     },
     "execution_count": 413,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(tf4_tweets.toarray(),columns = tfidf4.get_feature_names()).head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 414,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(500, 60)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>coffee</th>\n",
       "      <td>554</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lie</th>\n",
       "      <td>520</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>starbucks</th>\n",
       "      <td>489</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>rt</th>\n",
       "      <td>378</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fact</th>\n",
       "      <td>261</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>golfwriterkiel</th>\n",
       "      <td>261</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>built</th>\n",
       "      <td>261</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lots</th>\n",
       "      <td>260</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tastes</th>\n",
       "      <td>260</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>strong</th>\n",
       "      <td>260</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>brand</th>\n",
       "      <td>260</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>caffei</th>\n",
       "      <td>260</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fun</th>\n",
       "      <td>260</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>https</th>\n",
       "      <td>138</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>howardschultz</th>\n",
       "      <td>63</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dream</th>\n",
       "      <td>36</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>american</th>\n",
       "      <td>36</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>president</th>\n",
       "      <td>32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>schultz</th>\n",
       "      <td>29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>run</th>\n",
       "      <td>24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>andyostroy</th>\n",
       "      <td>24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020</th>\n",
       "      <td>22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>foot</th>\n",
       "      <td>21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>make</th>\n",
       "      <td>21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>family</th>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>clear</th>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>step</th>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>america</th>\n",
       "      <td>19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>wealth</th>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>represent</th>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>billionaires</th>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>obscene</th>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>nyc_erik</th>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>hoarding</th>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>workers</th>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>busting</th>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>infographic</th>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>impoverishment</th>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>yea</th>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ericdirnbach</th>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>regarding</th>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>relentless</th>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ceo</th>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>union</th>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>howard</th>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>just</th>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>trump</th>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>good</th>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>presidential</th>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>political</th>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>check</th>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>campaign</th>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cartoon</th>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>bid</th>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mariagrasmick</th>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>makes</th>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>does</th>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>grande</th>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ex</th>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>„Çπ„Çø„Éê</th>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                count\n",
       "coffee            554\n",
       "lie               520\n",
       "starbucks         489\n",
       "rt                378\n",
       "fact              261\n",
       "golfwriterkiel    261\n",
       "built             261\n",
       "lots              260\n",
       "tastes            260\n",
       "strong            260\n",
       "brand             260\n",
       "caffei            260\n",
       "fun               260\n",
       "https             138\n",
       "howardschultz      63\n",
       "dream              36\n",
       "american           36\n",
       "president          32\n",
       "schultz            29\n",
       "run                24\n",
       "andyostroy         24\n",
       "2020               22\n",
       "foot               21\n",
       "make               21\n",
       "family             20\n",
       "clear              20\n",
       "step               20\n",
       "america            19\n",
       "wealth             18\n",
       "represent          18\n",
       "billionaires       18\n",
       "obscene            18\n",
       "nyc_erik           18\n",
       "hoarding           18\n",
       "workers            18\n",
       "busting            16\n",
       "infographic        16\n",
       "impoverishment     16\n",
       "yea                16\n",
       "ericdirnbach       16\n",
       "regarding          16\n",
       "relentless         16\n",
       "ceo                16\n",
       "union              16\n",
       "howard             14\n",
       "just               13\n",
       "trump              13\n",
       "good               10\n",
       "presidential       10\n",
       "political           9\n",
       "check               9\n",
       "campaign            8\n",
       "cartoon             8\n",
       "bid                 8\n",
       "mariagrasmick       8\n",
       "makes               8\n",
       "does                8\n",
       "grande              8\n",
       "ex                  8\n",
       "„Çπ„Çø„Éê                 8"
      ]
     },
     "execution_count": 414,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# version 5 : \n",
    "\n",
    "#define the transformation\n",
    "\n",
    "cv5 = CountVectorizer(binary=False, min_df=.015,stop_words = \"english\")\n",
    "\n",
    "#apply the transformation\n",
    "cv5_tweets = cv5.fit_transform(tweet_frame['tweetText']) \n",
    "print(cv5_tweets.shape)\n",
    "\n",
    "names = cv5.get_feature_names()\n",
    "count = np.sum(cv5_tweets.toarray(), axis = 0) # add up feature counts \n",
    "count2 = count.tolist()  # convert numpy array to list\n",
    "count_df = pd.DataFrame(count2, index = names, columns = ['count'])\n",
    "count_df.sort_values(['count'], ascending = False)  #arrange by count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 415,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(500, 60)\n"
     ]
    }
   ],
   "source": [
    "#define the transformation with application of weights\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "tfidf5 = TfidfVectorizer(use_idf=True, norm=None,binary=False, min_df = .015, stop_words=\"english\")\n",
    "\n",
    "#apply the transformation on our retrieved tweets\n",
    "tf5_tweets = tfidf5.fit_transform(tweet_frame['tweetText']) \n",
    "print(tf5_tweets.shape)\n",
    "pd.DataFrame(tf5_tweets.toarray(),columns = tfidf5.get_feature_names())\n",
    "\n",
    "names = tfidf5.get_feature_names()\n",
    "\n",
    "#count = np.max(tf2_tweets.toarray(), axis = 0) # find the max weight of each feature \n",
    "count = np.sum(tf5_tweets.toarray(), axis = 0) # Weighted feature space \n",
    "# convert numpy array to list\n",
    "count2 = count.tolist()  # convert numpy array to list\n",
    " # create a dataframe from the list\n",
    "count_df = pd.DataFrame(count2, index = names, columns = ['count']) # create a dataframe from the list\n",
    "sorted_count = count_df.sort_values(['count'], ascending = False)  #arrange by count instead\n",
    "#print(sorted_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 416,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>2020</th>\n",
       "      <th>america</th>\n",
       "      <th>american</th>\n",
       "      <th>andyostroy</th>\n",
       "      <th>bid</th>\n",
       "      <th>billionaires</th>\n",
       "      <th>brand</th>\n",
       "      <th>built</th>\n",
       "      <th>busting</th>\n",
       "      <th>caffei</th>\n",
       "      <th>...</th>\n",
       "      <th>starbucks</th>\n",
       "      <th>step</th>\n",
       "      <th>strong</th>\n",
       "      <th>tastes</th>\n",
       "      <th>trump</th>\n",
       "      <th>union</th>\n",
       "      <th>wealth</th>\n",
       "      <th>workers</th>\n",
       "      <th>yea</th>\n",
       "      <th>„Çπ„Çø„Éê</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>1.098509</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>1.098509</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.652086</td>\n",
       "      <td>1.648262</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.652086</td>\n",
       "      <td>...</td>\n",
       "      <td>1.098509</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.652086</td>\n",
       "      <td>1.652086</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.652086</td>\n",
       "      <td>1.648262</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.652086</td>\n",
       "      <td>...</td>\n",
       "      <td>1.098509</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.652086</td>\n",
       "      <td>1.652086</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.652086</td>\n",
       "      <td>1.648262</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.652086</td>\n",
       "      <td>...</td>\n",
       "      <td>1.098509</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.652086</td>\n",
       "      <td>1.652086</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.652086</td>\n",
       "      <td>1.648262</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.652086</td>\n",
       "      <td>...</td>\n",
       "      <td>1.098509</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.652086</td>\n",
       "      <td>1.652086</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>4.081112</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.99773</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>1.098509</td>\n",
       "      <td>4.172084</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.652086</td>\n",
       "      <td>1.648262</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.652086</td>\n",
       "      <td>...</td>\n",
       "      <td>1.098509</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.652086</td>\n",
       "      <td>1.652086</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>4.081112</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>1.098509</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>9.303313</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.652086</td>\n",
       "      <td>1.648262</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.652086</td>\n",
       "      <td>...</td>\n",
       "      <td>1.098509</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.652086</td>\n",
       "      <td>1.652086</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10 rows √ó 60 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       2020  america  american  andyostroy  bid  billionaires     brand  \\\n",
       "0  0.000000      0.0       0.0     0.00000  0.0           0.0  0.000000   \n",
       "1  0.000000      0.0       0.0     0.00000  0.0           0.0  0.000000   \n",
       "2  0.000000      0.0       0.0     0.00000  0.0           0.0  1.652086   \n",
       "3  0.000000      0.0       0.0     0.00000  0.0           0.0  1.652086   \n",
       "4  0.000000      0.0       0.0     0.00000  0.0           0.0  1.652086   \n",
       "5  0.000000      0.0       0.0     0.00000  0.0           0.0  1.652086   \n",
       "6  4.081112      0.0       0.0     3.99773  0.0           0.0  0.000000   \n",
       "7  0.000000      0.0       0.0     0.00000  0.0           0.0  1.652086   \n",
       "8  4.081112      0.0       0.0     0.00000  0.0           0.0  0.000000   \n",
       "9  0.000000      0.0       0.0     0.00000  0.0           0.0  1.652086   \n",
       "\n",
       "      built  busting    caffei ...   starbucks      step    strong    tastes  \\\n",
       "0  0.000000      0.0  0.000000 ...    1.098509  0.000000  0.000000  0.000000   \n",
       "1  0.000000      0.0  0.000000 ...    1.098509  0.000000  0.000000  0.000000   \n",
       "2  1.648262      0.0  1.652086 ...    1.098509  0.000000  1.652086  1.652086   \n",
       "3  1.648262      0.0  1.652086 ...    1.098509  0.000000  1.652086  1.652086   \n",
       "4  1.648262      0.0  1.652086 ...    1.098509  0.000000  1.652086  1.652086   \n",
       "5  1.648262      0.0  1.652086 ...    1.098509  0.000000  1.652086  1.652086   \n",
       "6  0.000000      0.0  0.000000 ...    1.098509  4.172084  0.000000  0.000000   \n",
       "7  1.648262      0.0  1.652086 ...    1.098509  0.000000  1.652086  1.652086   \n",
       "8  0.000000      0.0  0.000000 ...    1.098509  0.000000  0.000000  0.000000   \n",
       "9  1.648262      0.0  1.652086 ...    1.098509  0.000000  1.652086  1.652086   \n",
       "\n",
       "      trump  union  wealth  workers  yea  „Çπ„Çø„Éê  \n",
       "0  0.000000    0.0     0.0      0.0  0.0  0.0  \n",
       "1  0.000000    0.0     0.0      0.0  0.0  0.0  \n",
       "2  0.000000    0.0     0.0      0.0  0.0  0.0  \n",
       "3  0.000000    0.0     0.0      0.0  0.0  0.0  \n",
       "4  0.000000    0.0     0.0      0.0  0.0  0.0  \n",
       "5  0.000000    0.0     0.0      0.0  0.0  0.0  \n",
       "6  0.000000    0.0     0.0      0.0  0.0  0.0  \n",
       "7  0.000000    0.0     0.0      0.0  0.0  0.0  \n",
       "8  9.303313    0.0     0.0      0.0  0.0  0.0  \n",
       "9  0.000000    0.0     0.0      0.0  0.0  0.0  \n",
       "\n",
       "[10 rows x 60 columns]"
      ]
     },
     "execution_count": 416,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(tf5_tweets.toarray(),columns = tfidf5.get_feature_names()).head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 417,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pd.DataFrame(tf5_tweets.toarray(),columns = tfidf5.get_feature_names()).tail(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Qs 2. We have made five versions of count vectorizer with different parameter settings. In the first version, we set binary parameter as true with all other default setting. We kept checking feature counts to see number of times a particular term has been used. Seeing output here, we realized that we needed to dissect 500 documents/tweets into four parts based on document frequency to extract features, their counting and weights closely in order to respond to our questionnaires. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In second vectorizer, we added ngram_range (1,2) to fetch all the occurrences with all combination of tokens. Feature space changed with the inclusion of unigram and bigram length tokens in the tweet. More in the parameter settings ‚Äì we kept minimum document frequency between .1 % to .5% and lowercase to False and default setting to none again. Setting lowercase to False did retrieve all kinds of features but wasn‚Äôt helpful for our quest. In the 3rd vectorizer, we looked into tokens with document frequency between .5% to .9% and eliminated stop words and kept lowercase to default. Eliminating stop words helped in removing unnecessary feature space without losing any insights. Here, I used term frequency(TF) with inverse document frequency to weight words according to how important they are or if we put simply, the higher the TFIDF score(weight), the rarer the term and vice versa and we find that terms were not important ones as nearly all scored low for 3rd vectorizer settings.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Moving forward to 4th vectorizer, we extracted all features with document frequency in between .9% to 1.5% with all other same settings. Again, we checked with TFIDF to know how important they were and found out that terms were very common with low score. Our last 5th vectorizer, we retrieved document frequency with 1.5% and above and keeping other settings same and our feature space is 60. In here, feature counting tells us that coffee is the most talk about term followed by lie, starbucks, retweet, brand, professor Kiel, tastes, fact and so on. I applied TFIDF on the 5th vectorizer settings to see word with high score or with frequent appearance in a document and we found that starbucks, coffee, tastes, strong, caffei, brand, built, retweet, howardschultz, people etc. are among highly weighted features. This last vectorizer seemed to be fetching all unique and relevant features in the tweets and useful feature space and application of weights helped us in scoring the importance of words (or ‚Äúterms‚Äù) in the document on how frequently they appear across multiple documents. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q3. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on our analysis, it does make sense to continue with the project however the challenge is that these topics keep changing in every couple of days and we need to observe a certain period of time to present solid analyzed report. Still, to some extent, there‚Äôs an answer to our current question  ‚Äì we could pull negative reviews about coffee beans and its process to be brewed by starbucks and its bitter taste. Also, people tweeting about their dissatisfaction on howard schultz as a republican candidate,  he belonged to starbucks.  People are obviously concerned about these topics and different demographics, mostly coming from the United States and as mentioned earlier different profession."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Going in detail on the dissection of tweets and extracting useful insights with enough basis to the tweets - People are tweeting about a fun coffee fact that the starbucks brand is built on a lie. There are 2,653 retweets on this topic, this could be considered as bad review and might impact the firm in the long run. There are tweets about football; politics ‚Äì democrats, republican, foxnews, and trump; Music ‚Äì chilling like a villain lyric; Shaun king ‚Äì a Brooklyn-based activist and writer who focuses on civil and human rights, racial justice, mass incarceration, and law enforcement misconduct and so on and so forth. We can see that people are taking interest, talking about various topics which they enjoy over a cup of coffee. Further, to analyze the tweets more effectively we can also include sentiment analysis.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#                         :The End:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
