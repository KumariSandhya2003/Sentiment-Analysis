{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test Assignment:2 - \n",
    "## Task1. \n",
    "### Exploratory Data Analysis :\n",
    "Number of words counted; Number of characters counted; Converted to lowercase; Removal of Punctuation; Customized replacement; Removal of Rare words;  Dropping duplicate tweets; Removed stop words; Customized stop words; Stemming; Lemmatization follwed by use of count vectorizer and tfidf vectorizer. I experimented with 4 versions of count vectorizer but settled on cv2 and cv3 for my analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A future statement must appear near the top of the module.\n",
    "from __future__ import division"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing module(s) into namespace\n",
    "import pandas as pd\n",
    "import numpy as np \n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "pathname = ''\n",
    "pd.set_option('display.max_colwidth', 15000) # maximum column for getting all the text\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before starting, let’s quickly read the tweets file from the dataset in order to perform different tasks on it. In the entire analysis, I'm using the twitter 'wall' dataset where I extracted tweets featuring #wall."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(500, 12)\n",
      "['tweetText', 'tweetRetweetCt', 'tweetFavoriteCt', 'tweetSource', 'tweetCreated', 'userScreen', 'userName', 'userCreateDt', 'userDesc', 'userFollowerCt', 'userFriendsCt', 'userLocation']\n"
     ]
    }
   ],
   "source": [
    "# Importing csv file\n",
    "tweetdf = pd.read_csv(pathname + \"wall.csv\", index_col = 0) \n",
    "print(tweetdf.shape)\n",
    "print(list(tweetdf)) #columns of a data frame\n",
    "#print(tweetdf['tweetText'][:6])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### tweetText column of tweet data frame is the center of attraction for my research work  here.\n",
    "One of the most basic features we can extract is the number of words in each tweet. The basic intuition behind this is that generally, the negative sentiments contain a lesser amount of words than the positive ones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of words in each tweet\n",
    "# using the split function in python\n",
    "tweetdf['word_count'] = tweetdf['tweetText'].apply(lambda x: len(str(x).split(\" \")))\n",
    "#tweetdf[['tweetText','word_count']].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Number of characters\n",
    "This feature is also based on the previous feature intuition. Here, calculating the number of characters in each tweet by calculating the length of the tweet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweetText</th>\n",
       "      <th>char_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>RT @Navy_Lady_45: Kamala,\\r\\n\\r\\nEveryone is laughing at your stupidity. The #Wall will be built and you will never be President.\\r\\n\\r\\n#Trump2020\\r\\n#Tr…</td>\n",
       "      <td>145</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>RT @John_KissMyBot: Trump Has The Authority To Declare A ‘National Emergency’ To Secure Our Border\\r\\n\\r\\nCongress Gave The President That Author…</td>\n",
       "      <td>142</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>In what alternate reality does John Bussey live? He keeps insisting that most Americans are against the #Wall and t… https://t.co/dTYqxiqalq</td>\n",
       "      <td>140</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>RT @JAMsMa: Dems want us to feel guilty for demanding a #Wall. How much guilt do they lay on illegals who come here to take what's OURS &amp;amp; s…</td>\n",
       "      <td>144</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>RT @RepCohen: Trump to take money for his #wall from #military schools,housing, and gyms saying “it didn’t seem that important to him.” And…</td>\n",
       "      <td>140</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                                     tweetText  \\\n",
       "0  RT @Navy_Lady_45: Kamala,\\r\\n\\r\\nEveryone is laughing at your stupidity. The #Wall will be built and you will never be President.\\r\\n\\r\\n#Trump2020\\r\\n#Tr…   \n",
       "1           RT @John_KissMyBot: Trump Has The Authority To Declare A ‘National Emergency’ To Secure Our Border\\r\\n\\r\\nCongress Gave The President That Author…   \n",
       "2                 In what alternate reality does John Bussey live? He keeps insisting that most Americans are against the #Wall and t… https://t.co/dTYqxiqalq   \n",
       "3             RT @JAMsMa: Dems want us to feel guilty for demanding a #Wall. How much guilt do they lay on illegals who come here to take what's OURS &amp; s…   \n",
       "4                 RT @RepCohen: Trump to take money for his #wall from #military schools,housing, and gyms saying “it didn’t seem that important to him.” And…   \n",
       "\n",
       "   char_count  \n",
       "0         145  \n",
       "1         142  \n",
       "2         140  \n",
       "3         144  \n",
       "4         140  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweetdf['char_count'] = tweetdf['tweetText'].str.len() ## this also includes spaces\n",
    "tweetdf[['tweetText','char_count']].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Next is basic preprocessing:\n",
    "My first pre-processing step is to transform tweets into lower case.\n",
    "This avoids having multiple copies of the same words. For example, while calculating the word count,\n",
    "‘National’ and ‘national’ had been taken as different words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0          rt @navy_lady_45: kamala, everyone is laughing at your stupidity. the #wall will be built and you will never be president. #trump2020 #tr…\n",
       "1         rt @john_kissmybot: trump has the authority to declare a ‘national emergency’ to secure our border congress gave the president that author…\n",
       "2        in what alternate reality does john bussey live? he keeps insisting that most americans are against the #wall and t… https://t.co/dtyqxiqalq\n",
       "3    rt @jamsma: dems want us to feel guilty for demanding a #wall. how much guilt do they lay on illegals who come here to take what's ours &amp; s…\n",
       "4        rt @repcohen: trump to take money for his #wall from #military schools,housing, and gyms saying “it didn’t seem that important to him.” and…\n",
       "Name: tweetText, dtype: object"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# converting to lowercase\n",
    "tweetdf['tweetText'] = tweetdf['tweetText'].apply(lambda x: \" \".join(x.lower() for x in x.split()))\n",
    "tweetdf['tweetText'].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The next step is to remove punctuation, as it doesn’t add any extra information while treating text data. \n",
    "Therefore removing all instances of it will help us reduce the size of the tweet data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removing punctuation\n",
    "\n",
    "# https://stackoverflow.com/questions/8376691/how-to-remove-hashtag-user-link-of-a-tweet-using-regular-expression\n",
    "# following code is borrowed from above link\n",
    "\n",
    "import re,string\n",
    "#('[^\\w\\s]','')\n",
    "def strip_links(text):\n",
    "    link_regex    =  re.compile('((https?):((//)|(\\\\\\\\))+([\\w\\d:#@%/;$()~_?\\+-=\\\\\\.&](#!)?)*)', re.DOTALL)\n",
    "    links         = re.findall(link_regex, text)\n",
    "    for link in links:\n",
    "        text = text.replace(link[0], ', ')    \n",
    "    return text\n",
    "\n",
    "def strip_all_entities(text):\n",
    "    entity_prefixes = ['@','#',\"\"]\n",
    "    for separator in  string.punctuation:\n",
    "        if separator not in entity_prefixes :\n",
    "            text = text.replace(separator,' ')\n",
    "    words = []\n",
    "    for word in text.split():\n",
    "        word = word.strip()\n",
    "        if word:\n",
    "            if word[0] not in entity_prefixes:\n",
    "                words.append(word)\n",
    "    return ' '.join(words)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0                              rt lady 45 kamala everyone is laughing at your stupidity the will be built and you will never be president\n",
      "1    rt kissmybot trump has the authority to declare a ‘national emergency’ to secure our border congress gave the president that author…\n",
      "2                           in what alternate reality does john bussey live he keeps insisting that most americans are against the and t…\n",
      "3          rt dems want us to feel guilty for demanding a how much guilt do they lay on illegals who come here to take what s ours amp s…\n",
      "4                        rt trump to take money for his from schools housing and gyms saying “it didn’t seem that important to him ” and…\n",
      "5                              rt lady 45 kamala everyone is laughing at your stupidity the will be built and you will never be president\n",
      "Name: tweetText, dtype: object\n"
     ]
    }
   ],
   "source": [
    "# applying the above function to tweetdf['tweetText] and keeping into tweetdf['punctuated']\n",
    "\n",
    "tweetdf['tweetText'] = tweetdf['tweetText'].map(lambda x:strip_all_entities(strip_links(x)))\n",
    "   \n",
    "print(tweetdf['tweetText'][0:6])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0                           rt lady 45 kamala everyone is laughing at your stupidity the will be built and you will never be president\n",
      "1    rt kissmybot trump has the authority to declare a national emergency to secure our border congress gave the president that author\n",
      "2                         in what alternate reality does john bussey live he keeps insisting that most americans are against the and t\n",
      "3        rt dems want us to feel guilty for demanding a how much guilt do they lay on illegals who come here to take what s ours amp s\n",
      "4                         rt trump to take money for his from schools housing and gyms saying it didnt seem that important to him  and\n",
      "5                           rt lady 45 kamala everyone is laughing at your stupidity the will be built and you will never be president\n",
      "Name: tweetText, dtype: object\n"
     ]
    }
   ],
   "source": [
    "# Now removing apostrophe and quotation marks \n",
    "\n",
    "tweetdf['tweetText'] = tweetdf['tweetText'].str.replace('[^\\w\\s]','')\n",
    "\n",
    "   \n",
    "print(tweetdf['tweetText'][0:6])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Custom Replacement:\n",
    "As we can see above output, we're left with didnt, isnt,aint etc.\n",
    "Therefore, customizing their replacement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# custom replacing contractions\n",
    "import re\n",
    "\n",
    "contractions_dict = {\"didnt\": \"did not\",\"isnt\":\"is not\",\"aint\": \"am not\",\"dont\":\"do not\",\"wont\":\"will not\", \"arent\":\"are not\"}\n",
    "   \n",
    "def multiple_replace(dict, text): \n",
    "\n",
    "  \"\"\" Replace in 'text' all occurences of any key in the given\n",
    "  dictionary by its corresponding value.  Returns the new tring.\"\"\" \n",
    "  text = str(text).lower()\n",
    "\n",
    "  # Creating a regular expression  from the dictionary keys\n",
    "  regex = re.compile(\"(%s)\" % \"|\".join(map(re.escape, dict.keys())))\n",
    "\n",
    "  # For each match, look-up corresponding value in dictionary\n",
    "  return regex.sub(lambda mo: dict[mo.string[mo.start():mo.end()]], text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0                           rt lady 45 kamala everyone is laughing at your stupidity the will be built and you will never be president\n",
       "1    rt kissmybot trump has the authority to declare a national emergency to secure our border congress gave the president that author\n",
       "2                         in what alternate reality does john bussey live he keeps insisting that most americans are against the and t\n",
       "3        rt dems want us to feel guilty for demanding a how much guilt do they lay on illegals who come here to take what s ours amp s\n",
       "4                       rt trump to take money for his from schools housing and gyms saying it did not seem that important to him  and\n",
       "5                           rt lady 45 kamala everyone is laughing at your stupidity the will be built and you will never be president\n",
       "Name: cleanedText, dtype: object"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweetdf['cleanedText'] = tweetdf['tweetText'].apply(lambda x: multiple_replace(contractions_dict, x))\n",
    "tweetdf['cleanedText'][0:6] "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Removing rarely occurring words from the text. Because they’re so rare, the association between them and other words is dominated by noise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "wrong     1\n",
       "plenty    1\n",
       "luxury    1\n",
       "wires     1\n",
       "mul       1\n",
       "dtype: int64"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Rare words removal\n",
    "\n",
    "freq = pd.Series(' '.join(tweetdf['cleanedText']).split()).value_counts()[-50:]\n",
    "freq.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removing some more\n",
    "freq = list(freq.index)\n",
    "tweetdf['cleanedText'] = tweetdf['cleanedText'].apply(lambda x: \" \".join(x for x in x.split() if x not in freq))\n",
    "#tweetdf['cleanedText'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "buying     1\n",
       "180cm      1\n",
       "circle     1\n",
       "tap        1\n",
       "customs    1\n",
       "dtype: int64"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "freq1 = pd.Series(' '.join(tweetdf['cleanedText']).split()).value_counts()[-50:]\n",
    "freq1.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0                           rt lady 45 kamala everyone is laughing at your stupidity the will be built and you will never be president\n",
       "1    rt kissmybot trump has the authority to declare a national emergency to secure our border congress gave the president that author\n",
       "2                         in what alternate reality does john bussey live he keeps insisting that most americans are against the and t\n",
       "3        rt dems want us to feel guilty for demanding a how much guilt do they lay on illegals who come here to take what s ours amp s\n",
       "4                        rt trump to take money for his from schools housing and gyms saying it did not seem that important to him and\n",
       "Name: cleanedText, dtype: object"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "freq2 = list(freq1.index)\n",
    "tweetdf['cleanedText'] = tweetdf['cleanedText'].apply(lambda x: \" \".join(x for x in x.split() if x not in freq))\n",
    "tweetdf['cleanedText'].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dropping duplicate tweets as they bring no new information to the dataset and are also computationally inefficient. Also, visualizing the most common words in the corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# most common words in twitter dataset\n",
    "all_words = []\n",
    "for line in list(tweetdf['cleanedText']):\n",
    "    words = line.split()\n",
    "    for word in words:\n",
    "        all_words.append(word.lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 864x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# plot word frequency distribution of first few words\n",
    "import nltk\n",
    "plt.figure(figsize=(12,5))\n",
    "plt.xticks(fontsize=13, rotation=90)\n",
    "fd = nltk.FreqDist(all_words)\n",
    "fd.plot(25,cumulative=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5,0,'Word Rank')"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 864x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# log-log of all words\n",
    "from collections import Counter\n",
    "word_counts = sorted(Counter(all_words).values(), reverse=True)\n",
    "plt.figure(figsize=(12,5))\n",
    "plt.loglog(word_counts, linestyle='-', linewidth=1.5)\n",
    "plt.ylabel(\"Freq\")\n",
    "plt.xlabel(\"Word Rank\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, I've got my cleaned data as we know all these pre-processing steps are essential and help us in reducing our vocabulary clutter so that the features produced in the end are more effective. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(500, 15)\n",
      "['tweetText', 'tweetRetweetCt', 'tweetFavoriteCt', 'tweetSource', 'tweetCreated', 'userScreen', 'userName', 'userCreateDt', 'userDesc', 'userFollowerCt', 'userFriendsCt', 'userLocation', 'word_count', 'char_count', 'cleanedText']\n"
     ]
    }
   ],
   "source": [
    "# the dimension changed with the addition of word and char count\n",
    "print(tweetdf.shape)\n",
    "print(list(tweetdf))\n",
    "#tweetdf['cleanedText'] # appended the results into original data frame data frame "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, moving towards Bag of Words - Vectorization. Bag of Words (BoW) refers to the representation of text which describes the presence of words within the text data. The intuition behind this is that two similar text fields will contain similar kind of words, and will therefore have a similar bag of words. Further, that from the text alone we can learn something about the meaning of the document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(500, 1096)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>to</th>\n",
       "      <td>346</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>the</th>\n",
       "      <td>331</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>rt</th>\n",
       "      <td>309</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>trump</th>\n",
       "      <td>184</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>and</th>\n",
       "      <td>171</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>that</th>\n",
       "      <td>144</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>for</th>\n",
       "      <td>133</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>national</th>\n",
       "      <td>119</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>president</th>\n",
       "      <td>113</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>emergency</th>\n",
       "      <td>112</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>it</th>\n",
       "      <td>94</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>not</th>\n",
       "      <td>91</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>our</th>\n",
       "      <td>90</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>of</th>\n",
       "      <td>86</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>from</th>\n",
       "      <td>83</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>border</th>\n",
       "      <td>78</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           count\n",
       "to           346\n",
       "the          331\n",
       "rt           309\n",
       "trump        184\n",
       "and          171\n",
       "that         144\n",
       "for          133\n",
       "national     119\n",
       "president    113\n",
       "emergency    112\n",
       "it            94\n",
       "not           91\n",
       "our           90\n",
       "of            86\n",
       "from          83\n",
       "border        78"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Instantiating the count vectorizer with all default parameter settings\n",
    "\n",
    "# Default settings – binary= False, lowercase = True, stop_words = None, max_df =1.0, min_df = 1. \n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "cv = CountVectorizer(binary=False) \n",
    "\n",
    "#apply the transformation\n",
    "cv_default = cv.fit_transform(tweetdf['cleanedText']) # bag of words\n",
    "print(cv_default.shape)\n",
    "\n",
    "# \"bag of words\"\n",
    "names = cv.get_feature_names()\n",
    "count = np.sum(cv_default.toarray(), axis = 0) # add up feature counts \n",
    "count2 = count.tolist()  # convert numpy array to list\n",
    "count_df = pd.DataFrame(count2, index = names, columns = ['count'])\n",
    "# following line is getting top 15 features by count instead of alphabetical\n",
    "count_df.sort_values(['count'], ascending = False)[0:16]  #arrange by count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see the above output, feature space has 1096 features where highest occurring word is 'to'-346,'the'-328,'rt'-309.\n",
    "\n",
    "Now, the top 1 and 2 are 'to' and 'the' these are considered as ‘Stop words’, which are the words that are so generic that we find in any sentences and we are not really interested in them. So we will remove these tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(500, 938)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>rt</th>\n",
       "      <td>309</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>trump</th>\n",
       "      <td>184</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>national</th>\n",
       "      <td>119</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>president</th>\n",
       "      <td>113</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>emergency</th>\n",
       "      <td>112</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>border</th>\n",
       "      <td>78</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>congress</th>\n",
       "      <td>68</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>money</th>\n",
       "      <td>65</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>secure</th>\n",
       "      <td>65</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gave</th>\n",
       "      <td>64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>declare</th>\n",
       "      <td>64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>authority</th>\n",
       "      <td>64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>author</th>\n",
       "      <td>63</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>kissmybot</th>\n",
       "      <td>63</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>important</th>\n",
       "      <td>62</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>saying</th>\n",
       "      <td>61</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           count\n",
       "rt           309\n",
       "trump        184\n",
       "national     119\n",
       "president    113\n",
       "emergency    112\n",
       "border        78\n",
       "congress      68\n",
       "money         65\n",
       "secure        65\n",
       "gave          64\n",
       "declare       64\n",
       "authority     64\n",
       "author        63\n",
       "kissmybot     63\n",
       "important     62\n",
       "saying        61"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# let's remove sklearn stop words\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# eliminating skl stop words \n",
    "cv1 = CountVectorizer(binary=False, stop_words = \"english\") \n",
    "cv1_skl_stopwords = cv1.fit_transform(tweetdf['cleanedText']) # bag of words\n",
    "print(cv1_skl_stopwords.shape)\n",
    "names = cv1.get_feature_names()   #create list of feature names\n",
    "count = np.sum(cv1_skl_stopwords.toarray(), axis = 0) # add up feature counts \n",
    "count2 = count.tolist()  # convert numpy array to list\n",
    "# following line is creating a dataframe consist of count column\n",
    "count_df = pd.DataFrame(count2, index = names, columns = ['count']) \n",
    "# following line is getting top 15 features by count instead of alphabetical\n",
    "count_df.sort_values(['count'], ascending = False)[0:16]  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>lady</th>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>check</th>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>people</th>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>son</th>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>stupidity</th>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>laughing</th>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>kamala</th>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>théâtre</th>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>urgence</th>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>invocation</th>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>trumps</th>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>grâce</th>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>va</th>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>drug</th>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>obtenir</th>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>etat</th>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            count\n",
       "lady            9\n",
       "check           8\n",
       "people          8\n",
       "son             8\n",
       "stupidity       8\n",
       "laughing        8\n",
       "kamala          8\n",
       "45              8\n",
       "théâtre         7\n",
       "urgence         7\n",
       "invocation      7\n",
       "trumps          7\n",
       "grâce           7\n",
       "va              7\n",
       "drug            7\n",
       "obtenir         7\n",
       "etat            7"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# feature space\n",
    "#cv1.get_feature_names()\n",
    "count_df.sort_values(['count'], ascending = False)[50:67] "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As it's visible in the above output,commonly occurring words have been removed in a general sense.\n",
    "Above vectorizer output is showing 939 unique features after removing stop words(english) to reduce redundant data which aren't helpful in getting insights.\n",
    "\n",
    "### Customized stop words\n",
    "\n",
    "Let's create a customized stop words to remove commonly occurring words from our text data.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction import text #import package\n",
    "\n",
    "skl_stopwords = text.ENGLISH_STOP_WORDS\n",
    "#print(skl_stopwords)\n",
    "\n",
    "from nltk.corpus import stopwords # for excluding the stopwords\n",
    "\n",
    "#creating an object using the default nltk stopwords\n",
    "nltk_stopwords = stopwords.words(\"english\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'set'> 179\n",
      "<class 'set'> 318\n"
     ]
    }
   ],
   "source": [
    "# Using sets to do intersections and unions\n",
    "# Creating two sets to work with\n",
    "\n",
    "set_a = set(nltk_stopwords) # create a set object from a list\n",
    "print(type(set_a), len(set_a))\n",
    "\n",
    "\n",
    "set_b = set(skl_stopwords) # create a set object from a list\n",
    "print(type(set_b), len(set_b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'set'> 378\n"
     ]
    }
   ],
   "source": [
    "#Find the intersection of our two sets and show the results\n",
    "set_c = set(set_a).union(set_b)\n",
    "print(type(set_c), len(set_c))\n",
    "#print(set_c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0                                   rt lady 45 kamala laughing stupidity built president\n",
       "1    rt trump authority declare national emergency secure border congress gave president\n",
       "2                           alternate reality john bussey live keeps insisting americans\n",
       "3                         rt dems want feel guilty demanding guilt lay illegals come amp\n",
       "4                                   rt trump money schools housing gyms saying important\n",
       "5                                   rt lady 45 kamala laughing stupidity built president\n",
       "Name: cleanedText, dtype: object"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#creating a custom stop words list by adding a few terms to the nltk list\n",
    " # did is still in feature space\n",
    "# \"author\" is representing authority only, \n",
    "# so adding  these in remove words words\n",
    "# I didn't like few word's presence so I removing\n",
    "removewords = set(['author','john_kissmybot','httpstcodtyqxiqalq','kissmybot'])\n",
    "my_stopwords = set_c.union(removewords) # all stop words in my_stopwords\n",
    "#my_stopwords\n",
    "# Appending removed my_stopwords in my cleaned data frame\n",
    "tweetdf['cleanedText'] = tweetdf['cleanedText'].apply(lambda x: \" \".join(x for x in x.split() if x not in my_stopwords))\n",
    "tweetdf['cleanedText'][:6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(500, 929)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>rt</th>\n",
       "      <td>309</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>trump</th>\n",
       "      <td>184</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>national</th>\n",
       "      <td>119</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>president</th>\n",
       "      <td>113</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>emergency</th>\n",
       "      <td>112</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>border</th>\n",
       "      <td>78</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>congress</th>\n",
       "      <td>68</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>money</th>\n",
       "      <td>65</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>secure</th>\n",
       "      <td>65</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gave</th>\n",
       "      <td>64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>authority</th>\n",
       "      <td>64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>declare</th>\n",
       "      <td>64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>important</th>\n",
       "      <td>62</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>saying</th>\n",
       "      <td>61</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gyms</th>\n",
       "      <td>60</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>schools</th>\n",
       "      <td>60</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>housing</th>\n",
       "      <td>60</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>wall</th>\n",
       "      <td>49</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>illegal</th>\n",
       "      <td>34</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>aclu</th>\n",
       "      <td>30</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           count\n",
       "rt           309\n",
       "trump        184\n",
       "national     119\n",
       "president    113\n",
       "emergency    112\n",
       "border        78\n",
       "congress      68\n",
       "money         65\n",
       "secure        65\n",
       "gave          64\n",
       "authority     64\n",
       "declare       64\n",
       "important     62\n",
       "saying        61\n",
       "gyms          60\n",
       "schools       60\n",
       "housing       60\n",
       "wall          49\n",
       "illegal       34\n",
       "aclu          30"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Now eliminaing customized stop words = my_stopwords\n",
    "cv2 = CountVectorizer(binary=False, stop_words = my_stopwords) \n",
    "\n",
    "cv2_custom_stopwords = cv2.fit_transform(tweetdf['cleanedText'])\n",
    "print(cv2_custom_stopwords.shape)\n",
    "names = cv2.get_feature_names()   #create list of feature names\n",
    "count = np.sum(cv2_custom_stopwords.toarray(), axis = 0) # add up feature counts \n",
    "count2 = count.tolist()  # convert numpy array to list\n",
    "# following line is creating a dataframe consist of count column\n",
    "count_df = pd.DataFrame(count2, index = names, columns = ['count']) \n",
    "# following line is getting top 20 features by count instead of alphabetical\n",
    "count_df.sort_values(['count'], ascending = False)[:20]  "
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "The output of vectorizer cv2: feature space has changed after removing all customized stop words and other parameter with default settings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature Space\n",
    "#cv2.get_feature_names() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(500, 19)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>rt</th>\n",
       "      <td>309</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>trump</th>\n",
       "      <td>184</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>national</th>\n",
       "      <td>119</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>president</th>\n",
       "      <td>113</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>emergency</th>\n",
       "      <td>112</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>border</th>\n",
       "      <td>78</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>congress</th>\n",
       "      <td>68</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>secure</th>\n",
       "      <td>65</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>money</th>\n",
       "      <td>65</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>authority</th>\n",
       "      <td>64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>declare</th>\n",
       "      <td>64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gave</th>\n",
       "      <td>64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>important</th>\n",
       "      <td>62</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>saying</th>\n",
       "      <td>61</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>schools</th>\n",
       "      <td>60</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>housing</th>\n",
       "      <td>60</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gyms</th>\n",
       "      <td>60</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>wall</th>\n",
       "      <td>49</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>illegal</th>\n",
       "      <td>34</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           count\n",
       "rt           309\n",
       "trump        184\n",
       "national     119\n",
       "president    113\n",
       "emergency    112\n",
       "border        78\n",
       "congress      68\n",
       "secure        65\n",
       "money         65\n",
       "authority     64\n",
       "declare       64\n",
       "gave          64\n",
       "important     62\n",
       "saying        61\n",
       "schools       60\n",
       "housing       60\n",
       "gyms          60\n",
       "wall          49\n",
       "illegal       34"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Now, looking at features that occurred more than 30 count.\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Hence adding minimum document frequency as 6.2% besides eliminaing customized stop words\n",
    "cv3 = CountVectorizer(binary = False, lowercase = False,min_df=0.062,stop_words = my_stopwords)\n",
    "\n",
    "cv3_maxdf = cv3.fit_transform(tweetdf['cleanedText'])\n",
    "print(cv3_maxdf.shape)\n",
    "names = cv3.get_feature_names()   #create list of feature names\n",
    "count = np.sum(cv3_maxdf.toarray(), axis = 0) # add up feature counts \n",
    "count2 = count.tolist()  # convert numpy array to list\n",
    "# following line is creating a dataframe consist of count column\n",
    "count_df = pd.DataFrame(count2, index = names, columns = ['count']) \n",
    "# following line is getting all features by count instead of alphabetical\n",
    "count_df.sort_values(['count'], ascending = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cv3 faeture space\n",
    "#cv3.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0                              rt ladi 45 kamala laugh stupid built presid\n",
      "1    rt trump author declar nation emerg secur border congress gave presid\n",
      "2                     altern realiti john bussey live keep insist american\n",
      "3                  rt dem want feel guilti demand guilt lay illeg come amp\n",
      "4                                rt trump money school hous gym say import\n",
      "Name: stemmed, dtype: object\n"
     ]
    }
   ],
   "source": [
    "# let's do some stemming \n",
    "\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "ps = PorterStemmer() \n",
    "\n",
    "def stem_text(row):\n",
    "    \n",
    "    # following line splits the text apart before stemming\n",
    "    text = str(row).split() \n",
    "    \n",
    "    #following line tells that we're applying porter stemmer\n",
    "    stemtext = [ps.stem(word) for word in text] \n",
    "    \n",
    "    # now putting everything back together in stemmedtext\n",
    "    stemmedtext = ' '.join(stemtext) \n",
    "    return stemmedtext\n",
    "\n",
    "# applying the above function to tweetdf['tweetText] and keeping into tweetdf['stemmed']\n",
    "tweetdf['stemmed'] = tweetdf['cleanedText'].apply(lambda x: stem_text(x)) \n",
    "#print( tweetdf.tweetText[0:5])\n",
    "#type(tweetdf['stemmed'])\n",
    "print(tweetdf['stemmed'][0:5])\n",
    "#print(tweetdf['stemmed'][493:497])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0                              rt ladi 45 kamala laugh stupid built presid\n",
       "1    rt trump author declar nation emerg secur border congress gave presid\n",
       "2                     altern realiti john bussey live keep insist american\n",
       "3                  rt dem want feel guilti demand guilt lay illeg come amp\n",
       "4                                rt trump money school hous gym say import\n",
       "5                              rt ladi 45 kamala laugh stupid built presid\n",
       "Name: cleanedText, dtype: object"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# appending the stemmed text in my cleaned data frame\n",
    "from nltk.stem import PorterStemmer\n",
    "st = PorterStemmer()\n",
    "tweetdf['cleanedText'][:6].apply(lambda x: \" \".join([st.stem(word) for word in x.split()]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(500, 827)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>rt</th>\n",
       "      <td>309</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>trump</th>\n",
       "      <td>191</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>nation</th>\n",
       "      <td>123</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>presid</th>\n",
       "      <td>114</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>emerg</th>\n",
       "      <td>113</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>border</th>\n",
       "      <td>78</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>declar</th>\n",
       "      <td>75</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>congress</th>\n",
       "      <td>68</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>say</th>\n",
       "      <td>67</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>secur</th>\n",
       "      <td>65</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>hous</th>\n",
       "      <td>65</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>money</th>\n",
       "      <td>65</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gave</th>\n",
       "      <td>64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>import</th>\n",
       "      <td>62</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gym</th>\n",
       "      <td>60</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>school</th>\n",
       "      <td>60</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          count\n",
       "rt          309\n",
       "trump       191\n",
       "nation      123\n",
       "presid      114\n",
       "emerg       113\n",
       "border       78\n",
       "declar       75\n",
       "congress     68\n",
       "say          67\n",
       "secur        65\n",
       "hous         65\n",
       "money        65\n",
       "gave         64\n",
       "import       62\n",
       "gym          60\n",
       "school       60"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Let's see application of cv2 on the stemmed text\n",
    "cv2_stemmed = cv2.fit_transform(tweetdf['stemmed'])\n",
    "print(cv2_stemmed.shape)\n",
    "names = cv2.get_feature_names()   \n",
    "count = np.sum(cv2_stemmed.toarray(), axis = 0) \n",
    "count2 = count.tolist()  \n",
    "count_df = pd.DataFrame(count2, index = names, columns = ['count'])\n",
    "#following line is getting top 15 features by count instead of alphabetical\n",
    "count_df.sort_values(['count'], ascending = False)[0:16]  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(500, 19)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>rt</th>\n",
       "      <td>309</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>trump</th>\n",
       "      <td>191</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>nation</th>\n",
       "      <td>123</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>presid</th>\n",
       "      <td>114</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>emerg</th>\n",
       "      <td>113</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>border</th>\n",
       "      <td>78</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>declar</th>\n",
       "      <td>75</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>congress</th>\n",
       "      <td>68</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>say</th>\n",
       "      <td>67</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>secur</th>\n",
       "      <td>65</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>hous</th>\n",
       "      <td>65</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>money</th>\n",
       "      <td>65</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gave</th>\n",
       "      <td>64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>import</th>\n",
       "      <td>62</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gym</th>\n",
       "      <td>60</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>school</th>\n",
       "      <td>60</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          count\n",
       "rt          309\n",
       "trump       191\n",
       "nation      123\n",
       "presid      114\n",
       "emerg       113\n",
       "border       78\n",
       "declar       75\n",
       "congress     68\n",
       "say          67\n",
       "secur        65\n",
       "hous         65\n",
       "money        65\n",
       "gave         64\n",
       "import       62\n",
       "gym          60\n",
       "school       60"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Let's see application of cv3 on the stemmed text\n",
    "cv3_stemmed = cv3.fit_transform(tweetdf['stemmed'])\n",
    "print(cv3_stemmed.shape)\n",
    "names = cv3.get_feature_names()   \n",
    "count = np.sum(cv3_stemmed.toarray(), axis = 0) \n",
    "count2 = count.tolist()  \n",
    "count_df = pd.DataFrame(count2, index = names, columns = ['count'])\n",
    "#following line is getting top 15 features by count instead of alphabetical\n",
    "count_df.sort_values(['count'], ascending = False)[0:16]  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparison of un-stemmed and stemmed text\n",
    "Un-stemmed text (cv2) had 930 features whereas Stemmed text(cv2) has 827 features.\n",
    "\n",
    "Un-stemmed text (cv3) had 19 features whereas Stemmed text(cv3) has 19 features.\n",
    "Stemmed text in first instance did good but in second instance, Stemmed text is as good as un-stemmed text!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's move to  Lemmatization #\n",
    "Lemmatization is a more effective option than stemming because it converts the word into its root word, rather than just stripping the suffices. It makes use of the vocabulary and does a morphological analysis to obtain the root word. Therefore, lemmatization is usually preferred  over stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 796,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0                                   rt lady 45 kamala laughing stupidity built president\n",
      "1    rt trump authority declare national emergency secure border congress gave president\n",
      "2                             alternate reality john bussey live keep insisting american\n",
      "3                         rt dems want feel guilty demanding guilt lay illegals come amp\n",
      "4                                     rt trump money school housing gym saying important\n",
      "5                                   rt lady 45 kamala laughing stupidity built president\n",
      "Name: lemmed, dtype: object\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#  wnl lemmatizer\n",
    "\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "wnl = WordNetLemmatizer()\n",
    "\n",
    "def lem_text(row):\n",
    "    \n",
    "    # following line splits the text apart before lemmatizing\n",
    "    text = str(row).split() \n",
    "        \n",
    "    #following line tells that we're applying porter lemmatizing\n",
    "    lemtext = [wnl.lemmatize(word) for word in text] \n",
    "    \n",
    "    # now putting everything back together in lemmedtext\n",
    "    lemmedtext = ' '.join(lemtext) \n",
    "    return lemmedtext\n",
    "\n",
    "# applying the above function to tweetdf['tweetText] and keeping into tweetdf['lemmed']\n",
    "tweetdf['lemmed'] = tweetdf['cleanedText'].apply(lambda x: lem_text(x)) \n",
    "print(tweetdf['lemmed'][0:6])\n",
    "\n",
    "#print(tweetdf['lemmed'][493:497])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 797,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0                                   rt lady 45 kamala laughing stupidity built president\n",
       "1    rt trump authority declare national emergency secure border congress gave president\n",
       "2                             alternate reality john bussey live keep insisting american\n",
       "3                         rt dems want feel guilty demanding guilt lay illegals come amp\n",
       "4                                     rt trump money school housing gym saying important\n",
       "5                                   rt lady 45 kamala laughing stupidity built president\n",
       "Name: cleanedText, dtype: object"
      ]
     },
     "execution_count": 797,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# appending the lemmatized text in my cleaned data frame\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "wnl = WordNetLemmatizer()\n",
    "\n",
    "\n",
    "tweetdf['cleanedText'][:6].apply(lambda x: \" \".join([wnl.lemmatize(word) for word in x.split()]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 805,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(500, 889)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>rt</th>\n",
       "      <td>309</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>trump</th>\n",
       "      <td>191</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>national</th>\n",
       "      <td>119</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>president</th>\n",
       "      <td>114</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>emergency</th>\n",
       "      <td>113</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>border</th>\n",
       "      <td>78</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>congress</th>\n",
       "      <td>68</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>money</th>\n",
       "      <td>65</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>secure</th>\n",
       "      <td>65</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>authority</th>\n",
       "      <td>64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>declare</th>\n",
       "      <td>64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gave</th>\n",
       "      <td>64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>important</th>\n",
       "      <td>62</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>saying</th>\n",
       "      <td>61</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>school</th>\n",
       "      <td>60</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>housing</th>\n",
       "      <td>60</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           count\n",
       "rt           309\n",
       "trump        191\n",
       "national     119\n",
       "president    114\n",
       "emergency    113\n",
       "border        78\n",
       "congress      68\n",
       "money         65\n",
       "secure        65\n",
       "authority     64\n",
       "declare       64\n",
       "gave          64\n",
       "important     62\n",
       "saying        61\n",
       "school        60\n",
       "housing       60"
      ]
     },
     "execution_count": 805,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# let's apply vectorization with the same parameter setting(cv2) on lemmatized text \n",
    "# and how it changes the feature space using this preprocessing\n",
    "\n",
    "cv2_lemmed = cv2.fit_transform(tweetdf['lemmed'])\n",
    "print(cv2_lemmed.shape)\n",
    "names = cv2.get_feature_names()   \n",
    "count = np.sum(cv2_lemmed.toarray(), axis = 0) \n",
    "count2 = count.tolist()  \n",
    "count_df = pd.DataFrame(count2, index = names, columns = ['count']) \n",
    "# following line is getting top 15 features by count instead of alphabetical\n",
    "count_df.sort_values(['count'], ascending = False)[0:16]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 799,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(500, 19)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>rt</th>\n",
       "      <td>309</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>trump</th>\n",
       "      <td>191</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>national</th>\n",
       "      <td>119</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>president</th>\n",
       "      <td>114</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>emergency</th>\n",
       "      <td>113</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>border</th>\n",
       "      <td>78</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>congress</th>\n",
       "      <td>68</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>secure</th>\n",
       "      <td>65</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>money</th>\n",
       "      <td>65</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>authority</th>\n",
       "      <td>64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>declare</th>\n",
       "      <td>64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gave</th>\n",
       "      <td>64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>important</th>\n",
       "      <td>62</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>saying</th>\n",
       "      <td>61</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>school</th>\n",
       "      <td>60</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>housing</th>\n",
       "      <td>60</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gym</th>\n",
       "      <td>60</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>wall</th>\n",
       "      <td>50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>illegal</th>\n",
       "      <td>34</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           count\n",
       "rt           309\n",
       "trump        191\n",
       "national     119\n",
       "president    114\n",
       "emergency    113\n",
       "border        78\n",
       "congress      68\n",
       "secure        65\n",
       "money         65\n",
       "authority     64\n",
       "declare       64\n",
       "gave          64\n",
       "important     62\n",
       "saying        61\n",
       "school        60\n",
       "housing       60\n",
       "gym           60\n",
       "wall          50\n",
       "illegal       34"
      ]
     },
     "execution_count": 799,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# let's apply vectorization with the same parameter setting(cv3) on lemmatized text \n",
    "# and how it changes the feature space using this preprocessing\n",
    "\n",
    "cv3_lemmed = cv3.fit_transform(tweetdf['lemmed'])\n",
    "print(cv3_lemmed.shape)\n",
    "names = cv3.get_feature_names()   \n",
    "count = np.sum(cv3_lemmed.toarray(), axis = 0) \n",
    "count2 = count.tolist()  \n",
    "count_df = pd.DataFrame(count2, index = names, columns = ['count']) \n",
    "# following line is getting all features by count instead of alphabetical\n",
    "count_df.sort_values(['count'], ascending = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparison of original vectorizer,stemmed and lemmatized text output\n",
    "\n",
    "Stemmed text(cv2) has 828 features whereas lemmatized text(cv2) has 889 features.\n",
    "Stemmed text(cv3) has 19 features whereas lemmatized text(cv3) has 19 features.\n",
    "\n",
    "Here, stemmed text is performing quite better than lemmatization when applying on the whole dataset while it's as good as lemmatized text, in the second instance where my bag of words are extracing words with minimum 6.2% document frequency.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### N-grams are the combination of multiple words used together. \n",
    "Ngrams with N=1 are called unigrams. Similarly, bigrams (N=2), trigrams (N=3) and so on can also be used. Next, I'm extracting bigrams from my cleaned tweet data using the ngrams range parameter to see any useful or valuable combination lying in there"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 800,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(500, 36)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>rt</th>\n",
       "      <td>309</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>trump</th>\n",
       "      <td>184</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>rt trump</th>\n",
       "      <td>122</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>national</th>\n",
       "      <td>119</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>president</th>\n",
       "      <td>113</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>emergency</th>\n",
       "      <td>112</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>national emergency</th>\n",
       "      <td>110</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>border</th>\n",
       "      <td>78</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>congress</th>\n",
       "      <td>68</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>money</th>\n",
       "      <td>65</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>secure</th>\n",
       "      <td>65</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>authority declare</th>\n",
       "      <td>64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>trump authority</th>\n",
       "      <td>64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>secure border</th>\n",
       "      <td>64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>authority</th>\n",
       "      <td>64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>emergency secure</th>\n",
       "      <td>64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>border congress</th>\n",
       "      <td>64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>congress gave</th>\n",
       "      <td>64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>declare</th>\n",
       "      <td>64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gave president</th>\n",
       "      <td>64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>declare national</th>\n",
       "      <td>64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gave</th>\n",
       "      <td>64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>important</th>\n",
       "      <td>62</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>saying</th>\n",
       "      <td>61</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>schools</th>\n",
       "      <td>60</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>trump money</th>\n",
       "      <td>60</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    count\n",
       "rt                    309\n",
       "trump                 184\n",
       "rt trump              122\n",
       "national              119\n",
       "president             113\n",
       "emergency             112\n",
       "national emergency    110\n",
       "border                 78\n",
       "congress               68\n",
       "money                  65\n",
       "secure                 65\n",
       "authority declare      64\n",
       "trump authority        64\n",
       "secure border          64\n",
       "authority              64\n",
       "emergency secure       64\n",
       "border congress        64\n",
       "congress gave          64\n",
       "declare                64\n",
       "gave president         64\n",
       "declare national       64\n",
       "gave                   64\n",
       "important              62\n",
       "saying                 61\n",
       "schools                60\n",
       "trump money            60"
      ]
     },
     "execution_count": 800,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Addition of ngram range as parameter in cv4\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# let's add ngram_range (1,2) to fetch all the occurrences of unigram and bigram length tokens in the tweet\n",
    "# also not to escape any meaningful combination in the content\n",
    "cv4 = CountVectorizer(binary=False,min_df=.062,ngram_range = (1,2), stop_words = my_stopwords) \n",
    "\n",
    "cv4_ngram = cv4.fit_transform(tweetdf['cleanedText'])\n",
    "print(cv4_ngram.shape)\n",
    "names = cv4.get_feature_names()   #create list of feature names\n",
    "count = np.sum(cv4_ngram.toarray(), axis = 0) # add up feature counts \n",
    "count2 = count.tolist()  # convert numpy array to list\n",
    "# following line is creating a dataframe consist of count column\n",
    "count_df = pd.DataFrame(count2, index = names, columns = ['count']) \n",
    "# following line is getting top 25 features by count instead of alphabetical\n",
    "count_df.sort_values(['count'], ascending = False)[0:26]  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Addition of ngram_range(1,2) doesn't look very useful, it's just creating repetition of words as can be seen in above vectorizer output. Feature space increased by 17 with the use of unigram and bigram length tokens while keeping rest of the parameter same as cv3. Therefore, I'd rather not use for it's not helping me in finding my insight."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--------------------------------------**-----------------------------------------------------**--------------------------------------------------------**------------------------------------------------**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 801,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0                                   rt lady 45 kamala laughing stupidity built president\n",
      "1    rt trump authority declare national emergency secure border congress gave president\n",
      "2                           alternate reality john bussey live keeps insisting americans\n",
      "3                         rt dems want feel guilty demanding guilt lay illegals come amp\n",
      "4                                   rt trump money schools housing gyms saying important\n",
      "5                                   rt lady 45 kamala laughing stupidity built president\n",
      "Name: cleanedText, dtype: object\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "print(tweetdf['cleanedText'][:6])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Term frequency is simply the ratio of the count of a word present in a sentence, to the length of the sentence.\n",
    "It denotes the contribution of the word to the document i.e words relevant to the document should be frequent. eg: A document about border, president,emergency should contain the word ‘border, president,emergency’ in large number."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 719,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>words</th>\n",
       "      <th>tf</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>national</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>secure</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>gave</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>border</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>president</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>congress</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>authority</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>emergency</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>declare</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>trump</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>rt</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        words  tf\n",
       "0    national   1\n",
       "1      secure   1\n",
       "2        gave   1\n",
       "3      border   1\n",
       "4   president   1\n",
       "5    congress   1\n",
       "6   authority   1\n",
       "7   emergency   1\n",
       "8     declare   1\n",
       "9       trump   1\n",
       "10         rt   1"
      ]
     },
     "execution_count": 719,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# term frequency table of a tweet\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "tf1 = (tweetdf['cleanedText'][1:2]).apply(lambda x: pd.value_counts(x.split(\" \"))).sum(axis = 0).reset_index()\n",
    "tf1.columns = ['words','tf']\n",
    "tf1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Above output is showing term frequency for second tweet in the data frame."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The intuition behind inverse document frequency (IDF) is that a word is not of much use to us if it’s appearing in all the documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 720,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>words</th>\n",
       "      <th>tf</th>\n",
       "      <th>idf</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>national</td>\n",
       "      <td>1</td>\n",
       "      <td>1.435485</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>secure</td>\n",
       "      <td>1</td>\n",
       "      <td>2.040221</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>gave</td>\n",
       "      <td>1</td>\n",
       "      <td>2.055725</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>border</td>\n",
       "      <td>1</td>\n",
       "      <td>1.845160</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>president</td>\n",
       "      <td>1</td>\n",
       "      <td>1.478410</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>congress</td>\n",
       "      <td>1</td>\n",
       "      <td>1.980502</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>authority</td>\n",
       "      <td>1</td>\n",
       "      <td>2.055725</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>emergency</td>\n",
       "      <td>1</td>\n",
       "      <td>1.496109</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>declare</td>\n",
       "      <td>1</td>\n",
       "      <td>1.966113</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>trump</td>\n",
       "      <td>1</td>\n",
       "      <td>0.967584</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>rt</td>\n",
       "      <td>1</td>\n",
       "      <td>0.379797</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        words  tf       idf\n",
       "0    national   1  1.435485\n",
       "1      secure   1  2.040221\n",
       "2        gave   1  2.055725\n",
       "3      border   1  1.845160\n",
       "4   president   1  1.478410\n",
       "5    congress   1  1.980502\n",
       "6   authority   1  2.055725\n",
       "7   emergency   1  1.496109\n",
       "8     declare   1  1.966113\n",
       "9       trump   1  0.967584\n",
       "10         rt   1  0.379797"
      ]
     },
     "execution_count": 720,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#  inverse document frequency (IDF) \n",
    "for i,word in enumerate(tf1['words']):\n",
    "    tf1.loc[i, 'idf'] = np.log(tweetdf.shape[0]/(len(tweetdf[tweetdf['cleanedText'].str.contains(word)])))\n",
    "\n",
    "tf1 \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 721,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>words</th>\n",
       "      <th>tf</th>\n",
       "      <th>idf</th>\n",
       "      <th>tfidf</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>national</td>\n",
       "      <td>1</td>\n",
       "      <td>1.435485</td>\n",
       "      <td>1.435485</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>secure</td>\n",
       "      <td>1</td>\n",
       "      <td>2.040221</td>\n",
       "      <td>2.040221</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>gave</td>\n",
       "      <td>1</td>\n",
       "      <td>2.055725</td>\n",
       "      <td>2.055725</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>border</td>\n",
       "      <td>1</td>\n",
       "      <td>1.845160</td>\n",
       "      <td>1.845160</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>president</td>\n",
       "      <td>1</td>\n",
       "      <td>1.478410</td>\n",
       "      <td>1.478410</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>congress</td>\n",
       "      <td>1</td>\n",
       "      <td>1.980502</td>\n",
       "      <td>1.980502</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>authority</td>\n",
       "      <td>1</td>\n",
       "      <td>2.055725</td>\n",
       "      <td>2.055725</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>emergency</td>\n",
       "      <td>1</td>\n",
       "      <td>1.496109</td>\n",
       "      <td>1.496109</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>declare</td>\n",
       "      <td>1</td>\n",
       "      <td>1.966113</td>\n",
       "      <td>1.966113</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>trump</td>\n",
       "      <td>1</td>\n",
       "      <td>0.967584</td>\n",
       "      <td>0.967584</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>rt</td>\n",
       "      <td>1</td>\n",
       "      <td>0.379797</td>\n",
       "      <td>0.379797</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        words  tf       idf     tfidf\n",
       "0    national   1  1.435485  1.435485\n",
       "1      secure   1  2.040221  2.040221\n",
       "2        gave   1  2.055725  2.055725\n",
       "3      border   1  1.845160  1.845160\n",
       "4   president   1  1.478410  1.478410\n",
       "5    congress   1  1.980502  1.980502\n",
       "6   authority   1  2.055725  2.055725\n",
       "7   emergency   1  1.496109  1.496109\n",
       "8     declare   1  1.966113  1.966113\n",
       "9       trump   1  0.967584  0.967584\n",
       "10         rt   1  0.379797  0.379797"
      ]
     },
     "execution_count": 721,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The more the value of IDF, the more unique is the word.\n",
    "tf1['tfidf'] = tf1['tf'] * tf1['idf']\n",
    "tf1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As can be seen that the TF-IDF has penalized words like ‘rt’and 'trump' because they're occurring more often. However, it has given a high weight to “authority” since that will be very useful in determining the sentiment of the tweet. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 722,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(500, 19)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>authority</th>\n",
       "      <th>border</th>\n",
       "      <th>congress</th>\n",
       "      <th>declare</th>\n",
       "      <th>emergency</th>\n",
       "      <th>gave</th>\n",
       "      <th>gyms</th>\n",
       "      <th>housing</th>\n",
       "      <th>illegal</th>\n",
       "      <th>important</th>\n",
       "      <th>money</th>\n",
       "      <th>national</th>\n",
       "      <th>president</th>\n",
       "      <th>rt</th>\n",
       "      <th>saying</th>\n",
       "      <th>schools</th>\n",
       "      <th>secure</th>\n",
       "      <th>trump</th>\n",
       "      <th>wall</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.858744</td>\n",
       "      <td>0.512404</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.343637</td>\n",
       "      <td>0.321603</td>\n",
       "      <td>0.336891</td>\n",
       "      <td>0.343637</td>\n",
       "      <td>0.281172</td>\n",
       "      <td>0.343637</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.274383</td>\n",
       "      <td>0.280177</td>\n",
       "      <td>0.167179</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.341912</td>\n",
       "      <td>0.226101</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.390111</td>\n",
       "      <td>0.390111</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.386058</td>\n",
       "      <td>0.382133</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.185907</td>\n",
       "      <td>0.388068</td>\n",
       "      <td>0.390111</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.251430</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   authority    border  congress   declare  emergency      gave      gyms  \\\n",
       "0   0.000000  0.000000  0.000000  0.000000   0.000000  0.000000  0.000000   \n",
       "1   0.343637  0.321603  0.336891  0.343637   0.281172  0.343637  0.000000   \n",
       "2   0.000000  0.000000  0.000000  0.000000   0.000000  0.000000  0.000000   \n",
       "3   0.000000  0.000000  0.000000  0.000000   0.000000  0.000000  0.000000   \n",
       "4   0.000000  0.000000  0.000000  0.000000   0.000000  0.000000  0.390111   \n",
       "\n",
       "    housing  illegal  important     money  national  president        rt  \\\n",
       "0  0.000000      0.0   0.000000  0.000000  0.000000   0.858744  0.512404   \n",
       "1  0.000000      0.0   0.000000  0.000000  0.274383   0.280177  0.167179   \n",
       "2  0.000000      0.0   0.000000  0.000000  0.000000   0.000000  0.000000   \n",
       "3  0.000000      0.0   0.000000  0.000000  0.000000   0.000000  1.000000   \n",
       "4  0.390111      0.0   0.386058  0.382133  0.000000   0.000000  0.185907   \n",
       "\n",
       "     saying   schools    secure     trump  wall  \n",
       "0  0.000000  0.000000  0.000000  0.000000   0.0  \n",
       "1  0.000000  0.000000  0.341912  0.226101   0.0  \n",
       "2  0.000000  0.000000  0.000000  0.000000   0.0  \n",
       "3  0.000000  0.000000  0.000000  0.000000   0.0  \n",
       "4  0.388068  0.390111  0.000000  0.251430   0.0  "
      ]
     },
     "execution_count": 722,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# the same result here for the entire data frame based on the same setting as count vectorizer 3\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "tfidf = TfidfVectorizer(lowercase=True, analyzer='word', stop_words= my_stopwords, min_df = 0.062)\n",
    "tfidf_tweet = tfidf.fit_transform(tweetdf['cleanedText'])\n",
    "print(tfidf_tweet.shape)\n",
    "tfidf_tweet\n",
    "\n",
    "pd.DataFrame(tfidf_tweet.toarray(),columns = tfidf.get_feature_names()).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First five rows depiction can be seen above.\n",
    "TFIDF things to remember:\n",
    "\n",
    "- Weight is highest when $t$ occurs many times within a small number of documents (thus lending high discriminating power to those documents)\n",
    "\n",
    "\n",
    "- Weight is lower when the term occurs fewer times in a document, or occurs in many documents (thus offering a less pronounced relevance signal)\n",
    "\n",
    "\n",
    "- Weight is lowest when the term occurs in virtually all documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 723,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(500, 19)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>authority</th>\n",
       "      <th>border</th>\n",
       "      <th>congress</th>\n",
       "      <th>declare</th>\n",
       "      <th>emergency</th>\n",
       "      <th>gave</th>\n",
       "      <th>gyms</th>\n",
       "      <th>housing</th>\n",
       "      <th>illegal</th>\n",
       "      <th>important</th>\n",
       "      <th>money</th>\n",
       "      <th>national</th>\n",
       "      <th>president</th>\n",
       "      <th>rt</th>\n",
       "      <th>saying</th>\n",
       "      <th>schools</th>\n",
       "      <th>secure</th>\n",
       "      <th>trump</th>\n",
       "      <th>wall</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>495</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>496</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.658549</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.642647</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.391558</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.658549</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.642647</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.391558</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>499</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.887279</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.461233</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     authority    border  congress  declare  emergency  gave  gyms  housing  \\\n",
       "495        0.0  0.000000       0.0      0.0   0.000000   0.0   0.0      0.0   \n",
       "496        0.0  0.000000       0.0      0.0   0.000000   0.0   0.0      0.0   \n",
       "497        0.0  0.000000       0.0      0.0   0.658549   0.0   0.0      0.0   \n",
       "498        0.0  0.000000       0.0      0.0   0.658549   0.0   0.0      0.0   \n",
       "499        0.0  0.887279       0.0      0.0   0.000000   0.0   0.0      0.0   \n",
       "\n",
       "     illegal  important  money  national  president        rt  saying  \\\n",
       "495      0.0        0.0    0.0  0.000000        0.0  0.000000     0.0   \n",
       "496      0.0        0.0    0.0  0.000000        0.0  0.000000     0.0   \n",
       "497      0.0        0.0    0.0  0.642647        0.0  0.391558     0.0   \n",
       "498      0.0        0.0    0.0  0.642647        0.0  0.391558     0.0   \n",
       "499      0.0        0.0    0.0  0.000000        0.0  0.461233     0.0   \n",
       "\n",
       "     schools  secure  trump  wall  \n",
       "495      0.0     0.0    1.0   0.0  \n",
       "496      0.0     0.0    0.0   0.0  \n",
       "497      0.0     0.0    0.0   0.0  \n",
       "498      0.0     0.0    0.0   0.0  \n",
       "499      0.0     0.0    0.0   0.0  "
      ]
     },
     "execution_count": 723,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# the same result here for the entire data frame based on the same setting as count vectorizer 3\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "tfidf = TfidfVectorizer(lowercase=True,use_idf = True, analyzer='word', stop_words= my_stopwords, min_df = 0.062)\n",
    "tfidf_tweet = tfidf.fit_transform(tweetdf['cleanedText'])\n",
    "print(tfidf_tweet.shape)\n",
    "tfidf_tweet\n",
    "\n",
    "pd.DataFrame(tfidf_tweet.toarray(),columns = tfidf.get_feature_names()).tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, last five rows result shows that border,national and emergency will be very useful in determining the sentiment of the tweet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 724,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                count\n",
      "rt         153.698467\n",
      "trump       60.532375\n",
      "national    46.213244\n",
      "president   45.217820\n",
      "emergency   41.865685\n",
      "wall        33.743461\n",
      "border      31.868775\n",
      "money       26.659939\n",
      "congress    25.565840\n",
      "important   25.071322\n",
      "saying      24.290969\n",
      "housing     23.413554\n",
      "gyms        23.413554\n",
      "schools     23.413554\n",
      "secure      22.615670\n",
      "gave        21.997657\n",
      "declare     21.997657\n",
      "authority   21.997657\n",
      "illegal     21.725252\n"
     ]
    }
   ],
   "source": [
    "names_tfidf = tfidf.get_feature_names()   #create list of tfidf feature names\n",
    "count = np.sum(tfidf_tweet.toarray(), axis = 0) # add up feature counts \n",
    "count2 = count.tolist()  # convert numpy array to list\n",
    "\n",
    "count_df = pd.DataFrame(count2, index = names_tfidf, columns = ['count']) # create a dataframe from the list\n",
    "sorted_count = count_df.sort_values(['count'], ascending = False)  #arrange by count instead\n",
    "print(sorted_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 725,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                count\n",
      "rt         153.698467\n",
      "trump       60.532375\n",
      "national    46.213244\n",
      "president   45.217820\n",
      "emergency   41.865685\n",
      "wall        33.743461\n",
      "border      31.868775\n",
      "money       26.659939\n",
      "congress    25.565840\n",
      "important   25.071322\n",
      "saying      24.290969\n",
      "housing     23.413554\n",
      "gyms        23.413554\n",
      "schools     23.413554\n",
      "secure      22.615670\n",
      "gave        21.997657\n",
      "declare     21.997657\n",
      "authority   21.997657\n",
      "illegal     21.725252\n"
     ]
    }
   ],
   "source": [
    "names_cv3 = cv3.get_feature_names()   #create list of feature names\n",
    "count = np.sum(cv3_maxdf.toarray(), axis = 0) # add up feature counts \n",
    "count2 = count.tolist()  # convert numpy array to list\n",
    "# following line is creating a dataframe consist of count column\n",
    "count_df = pd.DataFrame(count2, index = names_cv3, columns = ['count']) \n",
    "# following line is getting all 23features by count instead of alphabetical\n",
    "count_df.sort_values(['count'], ascending = False)\n",
    "print(sorted_count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As can be seen in above two ouputs, weitage count for count vectorizer and term frequency- inverse document frequency vectorizer are same."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q1. : \n",
    "  •\tAnswer:  For my 2nd assignment, I've decided to extract tweets featuring '#wall' as part of my job at a news network. Here, my executive has assigned me a task to find public opinion regarding building wall on southern border and what are they talking per se. They’re interested in understanding people sentiment for or against building the wall. So, my task is to do sentiment analysis and come up with top reasons why people believe in building the wall is good or bad for the America. Since, sentiment & emotion add important context to social media analysis, tweets are best indicator of one’s likes, and dislikes based on how many times a particular tweet has been retweeted. Sentiment analysis provide a window into individuals feelings about any topic. Similarly, the sentiments of people can be evaluated regarding building wall by dissecting the tweets to understand the views presented thru positive and negative statements. As part of my observation, people are targeting political personality which likely show their favor to a particular political party and eventually their support for wall. Also, they are sharing their points how this could be good or bad based on their social status and going to affect common people and economy. \n",
    " # ****************\n",
    "•\tFirst, I imported the wall.csv file where tweetText column of tweet data frame is being chosen to do sentiment analysis. I started with one of the most basic features that is the number of words in each tweet with the basic intuition of negative sentiments having lesser amount of words than the positive ones. Next, number of characters in each tweet is counted with the same intuition. In both cases, findings showed that 4th tweet is on top based on for first five tweets output. As part of preprocessing, data cleaning started with converting to lowercase which avoids having multiple copies of the same words followed by removing punctuation for it doesn’t add any extra information while treating text data, thereby, removing all instances of it will help us reduce the size of the tweet data.I had to customize few replacements in order to get desired result and removed rare words which wasn’t very meaningful to me. Next, duplicates tweets were dropped which could be work of twitter bots as we read in one of the articles. Removing stop words really helped in reducing our vocabulary clutter so that the features produced in the end are more effective. twitter messages are short by design and the above methods may not work so well because they essentially shorten words to their base words. Also, created customized stop words by combining sklearn stop words, nltk stop words and few words which I could see unnecessary repeated. At last, techniques like stemming and lemmatizing were not very useful on twitter data as twitter messages are short by design and these methods may not work so well because they essentially shorten words to their base words. \n",
    "# ****************\n",
    "•\tI’d definitely select built-in plus customized stop words as one of the fundamental preprocessing tasks. Few repeated words (httpstcodtyqxiqalq, john_kissmybot) with high occurrence count in my feature space couldn’t be removed even after doing entire preprocessing, so I added those in stop words. Punctuation removal did really help in clearing the picture but couldn’t get a perfect set of clean up as some cleanup steps introduced some flaws in the data removing, so I customized the way feature space collected useful tokens. Removal of rare words and dropping duplicate tweets also worked which I guess it worked but not very clear about it, I’d have to spend some time look closely. Stemming worked well when fetching all features but if I’m interested in high feature count then I’d not prefer it. Same goes to Lemmation based on my output."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------------------ ------------------------------------------------------------------------------------------\n",
    "###      T2. Appended data frame:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 503,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    RT @Navy_Lady_45: Kamala,\\r\\n\\r\\nEveryone is laughing at your stupidity. The #Wall will be built and you will never be President.\\r\\n\\r\\n#Trump2020\\r\\n#Tr…\n",
      "1             RT @John_KissMyBot: Trump Has The Authority To Declare A ‘National Emergency’ To Secure Our Border\\r\\n\\r\\nCongress Gave The President That Author…\n",
      "2                   In what alternate reality does John Bussey live? He keeps insisting that most Americans are against the #Wall and t… https://t.co/dTYqxiqalq\n",
      "3               RT @JAMsMa: Dems want us to feel guilty for demanding a #Wall. How much guilt do they lay on illegals who come here to take what's OURS &amp; s…\n",
      "4                   RT @RepCohen: Trump to take money for his #wall from #military schools,housing, and gyms saying “it didn’t seem that important to him.” And…\n",
      "5    RT @Navy_Lady_45: Kamala,\\r\\n\\r\\nEveryone is laughing at your stupidity. The #Wall will be built and you will never be President.\\r\\n\\r\\n#Trump2020\\r\\n#Tr…\n",
      "Name: tweetText, dtype: object\n"
     ]
    }
   ],
   "source": [
    "# Original text column in the tweet dataframe\n",
    "# first 5 rows\n",
    "#print(tweetdf['tweetText'][:6])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 506,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0          rt @navy_lady_45: kamala, everyone is laughing at your stupidity. the #wall will be built and you will never be president. #trump2020 #tr…\n",
      "1         rt @john_kissmybot: trump has the authority to declare a ‘national emergency’ to secure our border congress gave the president that author…\n",
      "2        in what alternate reality does john bussey live? he keeps insisting that most americans are against the #wall and t… https://t.co/dtyqxiqalq\n",
      "3    rt @jamsma: dems want us to feel guilty for demanding a #wall. how much guilt do they lay on illegals who come here to take what's ours &amp; s…\n",
      "4        rt @repcohen: trump to take money for his #wall from #military schools,housing, and gyms saying “it didn’t seem that important to him.” and…\n",
      "5          rt @navy_lady_45: kamala, everyone is laughing at your stupidity. the #wall will be built and you will never be president. #trump2020 #tr…\n",
      "Name: tweetText, dtype: object\n"
     ]
    }
   ],
   "source": [
    "# Converted to lowercase, Same rows in lowercase and difference can be seen\n",
    "#print(tweetdf['tweetText'][:6])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 544,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(500, 15)\n",
      "['tweetText', 'tweetRetweetCt', 'tweetFavoriteCt', 'tweetSource', 'tweetCreated', 'userScreen', 'userName', 'userCreateDt', 'userDesc', 'userFollowerCt', 'userFriendsCt', 'userLocation', 'word_count', 'char_count', 'cleanedText']\n",
      "0                           rt lady 45 kamala everyone is laughing at your stupidity the will be built and you will never be president\n",
      "1    rt kissmybot trump has the authority to declare a national emergency to secure our border congress gave the president that author\n",
      "2                         in what alternate reality does john bussey live he keeps insisting that most americans are against the and t\n",
      "3        rt dems want us to feel guilty for demanding a how much guilt do they lay on illegals who come here to take what s ours amp s\n",
      "4                        rt trump to take money for his from schools housing and gyms saying it did not seem that important to him and\n",
      "5                           rt lady 45 kamala everyone is laughing at your stupidity the will be built and you will never be president\n",
      "Name: cleanedText, dtype: object\n"
     ]
    }
   ],
   "source": [
    "#I’ve been appending while doing preprocessing:\n",
    "# first 5 rows\n",
    "# the dimension changed with the addition of word and char count\n",
    "print(tweetdf.shape)\n",
    "print(list(tweetdf))\n",
    "#tweetdf['cleanedText'] # appended the results into original data frame data frame \n",
    "#print(tweetdf['cleanedText'][:6])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### I had to face mishap when I stemmed my data before cleaning my text which I removed in my final output.\n",
    "### final output is my success !\n",
    "For example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 546,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0                         rt ladi 45 kamala everyon is laugh at your stupid the will be built and you will never be presid\n",
      "1       rt kissmybot trump ha the author to declar a nation emerg to secur our border congress gave the presid that author\n",
      "2                      in what altern realiti doe john bussey live he keep insist that most american are against the and t\n",
      "3    rt dem want us to feel guilti for demand a how much guilt do they lay on illeg who come here to take what s our amp s\n",
      "4                          rt trump to take money for hi from school hous and gym say it didnt seem that import to him and\n",
      "Name: stemmed, dtype: object\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem.porter import PorterStemmer\n",
    "ps = PorterStemmer() \n",
    "\n",
    "def stem_text(row):\n",
    "    \n",
    "    # following line splits the text apart before stemming\n",
    "    text = str(row).split() \n",
    "    \n",
    "    #following line tells that we're applying porter stemmer\n",
    "    stemtext = [ps.stem(word) for word in text] \n",
    "    \n",
    "    # now putting everything back together in stemmedtext\n",
    "    stemmedtext = ' '.join(stemtext) \n",
    "    return stemmedtext\n",
    "\n",
    "# applying the above function to tweetdf['tweetText] and keeping into tweetdf['stemmed']\n",
    "tweetdf['stemmed'] = tweetdf['tweetText'].apply(lambda x: stem_text(x)) \n",
    "#print( tweetdf.tweetText[0:5])\n",
    "#type(tweetdf['stemmed'])\n",
    "#print(tweetdf['stemmed'][0:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 547,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(500, 899)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>rt</th>\n",
       "      <td>309</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>trump</th>\n",
       "      <td>191</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>nation</th>\n",
       "      <td>123</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>presid</th>\n",
       "      <td>114</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>emerg</th>\n",
       "      <td>113</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>border</th>\n",
       "      <td>78</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>hi</th>\n",
       "      <td>77</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>declar</th>\n",
       "      <td>75</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ha</th>\n",
       "      <td>74</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>congress</th>\n",
       "      <td>68</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>say</th>\n",
       "      <td>67</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>money</th>\n",
       "      <td>65</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>secur</th>\n",
       "      <td>65</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>hous</th>\n",
       "      <td>65</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gave</th>\n",
       "      <td>64</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          count\n",
       "rt          309\n",
       "trump       191\n",
       "nation      123\n",
       "presid      114\n",
       "emerg       113\n",
       "border       78\n",
       "hi           77\n",
       "declar       75\n",
       "ha           74\n",
       "congress     68\n",
       "say          67\n",
       "money        65\n",
       "secur        65\n",
       "hous         65\n",
       "gave         64"
      ]
     },
     "execution_count": 547,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# how does that affect the feature list\n",
    "\n",
    "cv2_dm = cv2.fit_transform(tweetdf['stemmed'])\n",
    "print(cv2_dm.shape)\n",
    "names = cv2.get_feature_names()   \n",
    "count = np.sum(cv2_dm.toarray(), axis = 0) \n",
    "count2 = count.tolist()  \n",
    "count_df = pd.DataFrame(count2, index = names, columns = ['count'])\n",
    "#following line is getting top 15 features by count instead of alphabetical\n",
    "#count_df.sort_values(['count'], ascending = False)[0:15]  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Even If I did custom replacment for these words, that wouldn't be meaningful as can be seen below :) \n",
    "\n",
    "So, I'd just ignore these words from my analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 548,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "stem_dict = { 'hi':' his', 'ha':'has','thi':'this'}\n",
    "\n",
    "def multiple_replace(dict, text): \n",
    "\n",
    "  \"\"\" Replace in 'text' all occurences of any key in the given\n",
    "  dictionary by its corresponding value.  Returns the new tring.\"\"\" \n",
    "  text = str(text).lower()\n",
    "\n",
    "  # Create a regular expression  from the dictionary keys\n",
    "  regex = re.compile(\"(%s)\" % \"|\".join(map(re.escape, dict.keys())))\n",
    "\n",
    "  # For each match, look-up corresponding value in dictionary\n",
    "  return regex.sub(lambda mo: dict[mo.string[mo.start():mo.end()]], text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 549,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0                           rt lady 45 kamala everyone is laug hisng at your stupidity the will be built and you will never be president\n",
       "1    rt kissmybot trump hass the authority to declare a national emergency to secure our border congress gave the president thast author\n",
       "2                         in whast alternate reality does john bussey live he keeps insisting thast most americans are against the and t\n",
       "3         rt dems want us to feel guilty for demanding a how much guilt do they lay on illegals who come here to take whast s ours amp s\n",
       "4                      rt trump to take money for  hiss from schools housing and gyms saying it didnt seem thast important to  hism  and\n",
       "Name: stemmedtext, dtype: object"
      ]
     },
     "execution_count": 549,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweetdf['stemmedtext'] = tweetdf['tweetText'].apply(lambda x: multiple_replace(stem_dict, x))\n",
    "#tweetdf['stemmedtext'][0:5] "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Few ones to notice 'that' became thast,'what' = whast,'him' = hism,'has' = hass, 'his' = hiss   \n",
    "### Silly, isn't it? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 551,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(500, 999)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>rt</th>\n",
       "      <td>309</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>trump</th>\n",
       "      <td>184</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>thast</th>\n",
       "      <td>144</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>national</th>\n",
       "      <td>119</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>president</th>\n",
       "      <td>113</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>emergency</th>\n",
       "      <td>112</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>border</th>\n",
       "      <td>78</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>hiss</th>\n",
       "      <td>77</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>hass</th>\n",
       "      <td>74</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>congress</th>\n",
       "      <td>68</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>secure</th>\n",
       "      <td>65</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>money</th>\n",
       "      <td>65</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gave</th>\n",
       "      <td>64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>authority</th>\n",
       "      <td>64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>declare</th>\n",
       "      <td>64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>kissmybot</th>\n",
       "      <td>63</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>important</th>\n",
       "      <td>62</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>saying</th>\n",
       "      <td>61</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>didnt</th>\n",
       "      <td>60</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>hism</th>\n",
       "      <td>60</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gyms</th>\n",
       "      <td>60</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>schools</th>\n",
       "      <td>60</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>housing</th>\n",
       "      <td>60</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>wall</th>\n",
       "      <td>49</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>thiss</th>\n",
       "      <td>45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>illegal</th>\n",
       "      <td>34</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lawsuit</th>\n",
       "      <td>30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>announces</th>\n",
       "      <td>30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>aclu</th>\n",
       "      <td>30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dems</th>\n",
       "      <td>22</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           count\n",
       "rt           309\n",
       "trump        184\n",
       "thast        144\n",
       "national     119\n",
       "president    113\n",
       "emergency    112\n",
       "border        78\n",
       "hiss          77\n",
       "hass          74\n",
       "congress      68\n",
       "secure        65\n",
       "money         65\n",
       "gave          64\n",
       "authority     64\n",
       "declare       64\n",
       "kissmybot     63\n",
       "important     62\n",
       "saying        61\n",
       "didnt         60\n",
       "hism          60\n",
       "gyms          60\n",
       "schools       60\n",
       "housing       60\n",
       "wall          49\n",
       "thiss         45\n",
       "illegal       34\n",
       "lawsuit       30\n",
       "announces     30\n",
       "aclu          30\n",
       "dems          22"
      ]
     },
     "execution_count": 551,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Still let's see change of the feature space using this preprocessing\n",
    "\n",
    "cv2_dm = cv2.fit_transform(tweetdf['stemmedtext'])\n",
    "print(cv2_dm.shape)\n",
    "names = cv2.get_feature_names()   \n",
    "count = np.sum(cv2_dm.toarray(), axis = 0) \n",
    "count2 = count.tolist()  \n",
    "count_df = pd.DataFrame(count2, index = names, columns = ['count']) \n",
    "#count_df.sort_values(['count'], ascending = False)[0:30] "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparing cv2 on both stemmed text\n",
    "Vectorizer output and feature space:\n",
    "using tweetdf['stemmed'] = 899\n",
    "using tweetdf['stemmedtext'] = 999\n",
    "It didn't improve feature space instead feature space increased by 100!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q2.\n",
    "•\tYes, I’d say results did reflect my expectation based on my preprocessing selection. Firstly, transforming tweets into lower case helped avoiding having multiple copies of the same words, for example, while calculating the word count, ‘Emergency’ and ‘emergency’ were taken as different words, thus, it reduced repetition. Secondly, using punctuation removal really eliminated all special characters, quotation marks plus URL links and third was stop word removal made a quite a presence in the feature space and common words were almost gone. Stemming is useful when I’m considering features with low document frequency and deep cleaning but if I’m interested in only top occurring words, stemming might not be helpful and lemmatization can be more useful with the use of POS tagging.\n",
    "•\tI observed mishap when I stemmed my data before cleaning my text (on original data) where stemming output created few words such as hi, ha, thi which didn't make sense. Their presence in the feature space weren't useful for my research per se and even I tried customizing replacing them but of no use. Hence, I’ve started afresh with cleansing of data first and keep appending data frame and stemmed on the cleaned data and it depicted what I needed. Other mishaps I encountered are few irrelevant tokens such as 'john_kissmybot','httpstcodtyqxiqalq','kissmybot' etc. They occurred so many times that they were coming on top feature list and so I went ahead and included them in stop words, haven’t got any better option yet. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**------------------------------------**-----------------------------**----------------------------**-------------------------------**\n",
    "#### T3 :\n",
    "Sentiment dictionary used here are afinn,hl_sent, Gi dictionary, and afinn2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 729,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'dict'> 2477\n",
      "abandon => -2\n",
      "abandoned => -2\n",
      "abandons => -2\n",
      "abducted => -2\n",
      "abduction => -2\n",
      "abductions => -2\n",
      "abhor => -3\n",
      "abhorred => -3\n",
      "abhorrent => -3\n",
      "abhors => -3\n",
      "~~~~~~~~~~~~\n",
      "yeah => 1\n",
      "yearning => 1\n",
      "yeees => 2\n",
      "yes => 1\n",
      "youthful => 2\n",
      "yucky => -2\n",
      "yummy => 3\n",
      "zealot => -2\n",
      "zealots => -2\n",
      "zealous => 2\n"
     ]
    }
   ],
   "source": [
    "# Using afinn dictionaries to assign a value \n",
    "afinn = {}\n",
    "for line in open('C:/Users/email/Downloads/'+\"AFINN-111.txt\"):\n",
    "    tt = line.split('\\t')\n",
    "    afinn.update({tt[0]:int(tt[1])})\n",
    "\n",
    "\n",
    "print(type(afinn), len(afinn))\n",
    "\n",
    "for key, value in sorted(afinn.items())[0:10]:\n",
    "    print(key + \" => \" + str(value))\n",
    "print(\"~~~~~~~~~~~~\")\n",
    "for key, value in sorted(afinn.items())[2467:]:\n",
    "    print(key + \" => \" + str(value))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 730,
   "metadata": {},
   "outputs": [],
   "source": [
    "# here we are going for strictly the sum:  add up the positives and \"subtract\" the negatives\n",
    "# returning a label\n",
    "\n",
    "def afinn_sent(inputstring):\n",
    "    \n",
    "    sentcount =0\n",
    "    for word in inputstring.split():  \n",
    "        if word.rstrip('?:!.,;') in afinn:\n",
    "            sentcount = sentcount + afinn[word.rstrip('?:!.,;')]\n",
    "            \n",
    "    \n",
    "    if (sentcount < 0):\n",
    "        sentiment = 'Negative'\n",
    "    elif (sentcount >0):\n",
    "        sentiment = 'Positive'\n",
    "    else:\n",
    "        sentiment = 'Neutral'\n",
    "    \n",
    "    return sentiment\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 731,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.series.Series'>\n"
     ]
    }
   ],
   "source": [
    "tweetdf['afinn'] = tweetdf.tweetText.apply(lambda x: afinn_sent(x))\n",
    "print(type(tweetdf.tweetText))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 732,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweetText</th>\n",
       "      <th>afinn</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>rt lady 45 kamala everyone is laughing at your stupidity the will be built and you will never be president</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>rt kissmybot trump has the authority to declare a national emergency to secure our border congress gave the president that author</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>in what alternate reality does john bussey live he keeps insisting that most americans are against the and t</td>\n",
       "      <td>Neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>rt dems want us to feel guilty for demanding a how much guilt do they lay on illegals who come here to take what s ours amp s</td>\n",
       "      <td>Negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>rt trump to take money for his from schools housing and gyms saying it didnt seem that important to him  and</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                           tweetText  \\\n",
       "0                         rt lady 45 kamala everyone is laughing at your stupidity the will be built and you will never be president   \n",
       "1  rt kissmybot trump has the authority to declare a national emergency to secure our border congress gave the president that author   \n",
       "2                       in what alternate reality does john bussey live he keeps insisting that most americans are against the and t   \n",
       "3      rt dems want us to feel guilty for demanding a how much guilt do they lay on illegals who come here to take what s ours amp s   \n",
       "4                       rt trump to take money for his from schools housing and gyms saying it didnt seem that important to him  and   \n",
       "\n",
       "      afinn  \n",
       "0  Positive  \n",
       "1  Positive  \n",
       "2   Neutral  \n",
       "3  Negative  \n",
       "4  Positive  "
      ]
     },
     "execution_count": 732,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweetdf.iloc[0:5][['tweetText','afinn']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 733,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.series.Series'>\n"
     ]
    }
   ],
   "source": [
    "tweetdf['afinn'] = tweetdf.cleanedText.apply(lambda x: afinn_sent(x))\n",
    "print(type(tweetdf.cleanedText))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 734,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cleanedText</th>\n",
       "      <th>afinn</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>rt lady 45 kamala laughing stupidity built president</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>rt trump authority declare national emergency secure border congress gave president</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>alternate reality john bussey live keeps insisting americans</td>\n",
       "      <td>Neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>rt dems want feel guilty demanding guilt lay illegals come amp</td>\n",
       "      <td>Negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>rt trump money schools housing gyms saying important</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                           cleanedText  \\\n",
       "0                                 rt lady 45 kamala laughing stupidity built president   \n",
       "1  rt trump authority declare national emergency secure border congress gave president   \n",
       "2                         alternate reality john bussey live keeps insisting americans   \n",
       "3                       rt dems want feel guilty demanding guilt lay illegals come amp   \n",
       "4                                 rt trump money schools housing gyms saying important   \n",
       "\n",
       "      afinn  \n",
       "0  Positive  \n",
       "1  Positive  \n",
       "2   Neutral  \n",
       "3  Negative  \n",
       "4  Positive  "
      ]
     },
     "execution_count": 734,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweetdf.iloc[0:5][['cleanedText','afinn']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### I tried afinn sentiments on both original and cleaned text and found out that results are same fo first five rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 735,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HL pos  size: 2006\n",
      "['a+', 'abound', 'abounds', 'abundance', 'abundant', 'accessable', 'accessible', 'acclaim', 'acclaimed', 'acclamation']\n",
      "HL neg  size: 4783\n",
      "['2-faced', '2-faces', 'abnormal', 'abolish', 'abominable', 'abominably', 'abominate', 'abomination', 'abort', 'aborted']\n"
     ]
    }
   ],
   "source": [
    "#some sort into buckets\n",
    "HLpos = [line.strip() for line in  open('C:/Users/email/Downloads/'+'HLpos.txt','r')]\n",
    "HLneg = [line.strip() for line in  open('C:/Users/email/Downloads/' +'HLneg.txt','r',encoding = 'latin-1')]\n",
    "print(\"HL pos  size: \" + str(len(HLpos)))\n",
    "print(HLpos[0:10])\n",
    "print(\"HL neg  size: \" + str(len(HLneg)))\n",
    "print(HLneg[0:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 736,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# trying a different measure using a different dictionary\n",
    "\n",
    "def hl_sent(inputstring):\n",
    "\n",
    "    poscount = 0\n",
    "    negcount = 0\n",
    "    \n",
    "    for word in inputstring.split(): \n",
    "        if HLpos.count(word.rstrip('?:!.,;')):\n",
    "            poscount +=1\n",
    "        elif HLneg.count(word.rstrip('?:!.,;')):\n",
    "            negcount +=1\n",
    "     \n",
    "    \n",
    "    if poscount+negcount > 0:\n",
    "        t = float((poscount - negcount)/(poscount+negcount))    \n",
    "    else:\n",
    "        t = 0\n",
    "    \n",
    "    \n",
    "    if t > 0:\n",
    "        tone = \"Positive\"\n",
    "    elif t < 0:\n",
    "        tone = \"Negative\"\n",
    "    else:\n",
    "        tone = \"Neutral\"\n",
    "    \n",
    "    return tone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 737,
   "metadata": {},
   "outputs": [],
   "source": [
    "#takes a little longer - needs to check multiple lists\n",
    "tweetdf['hlsent'] = tweetdf.cleanedText.apply(lambda x: hl_sent(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 738,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cleanedText</th>\n",
       "      <th>afinn</th>\n",
       "      <th>hlsent</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>rt lady 45 kamala laughing stupidity built president</td>\n",
       "      <td>Positive</td>\n",
       "      <td>Negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>rt trump authority declare national emergency secure border congress gave president</td>\n",
       "      <td>Positive</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>alternate reality john bussey live keeps insisting americans</td>\n",
       "      <td>Neutral</td>\n",
       "      <td>Neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>rt dems want feel guilty demanding guilt lay illegals come amp</td>\n",
       "      <td>Negative</td>\n",
       "      <td>Negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>rt trump money schools housing gyms saying important</td>\n",
       "      <td>Positive</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>rt lady 45 kamala laughing stupidity built president</td>\n",
       "      <td>Positive</td>\n",
       "      <td>Negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>want wants issue wall built think tha</td>\n",
       "      <td>Positive</td>\n",
       "      <td>Negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>rt trump money schools housing gyms saying important</td>\n",
       "      <td>Positive</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>rt la révolution est féministe come</td>\n",
       "      <td>Neutral</td>\n",
       "      <td>Neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>rt trump money schools housing gyms saying important</td>\n",
       "      <td>Positive</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                           cleanedText  \\\n",
       "0                                 rt lady 45 kamala laughing stupidity built president   \n",
       "1  rt trump authority declare national emergency secure border congress gave president   \n",
       "2                         alternate reality john bussey live keeps insisting americans   \n",
       "3                       rt dems want feel guilty demanding guilt lay illegals come amp   \n",
       "4                                 rt trump money schools housing gyms saying important   \n",
       "5                                 rt lady 45 kamala laughing stupidity built president   \n",
       "6                                                want wants issue wall built think tha   \n",
       "7                                 rt trump money schools housing gyms saying important   \n",
       "8                                                  rt la révolution est féministe come   \n",
       "9                                 rt trump money schools housing gyms saying important   \n",
       "\n",
       "      afinn    hlsent  \n",
       "0  Positive  Negative  \n",
       "1  Positive  Positive  \n",
       "2   Neutral   Neutral  \n",
       "3  Negative  Negative  \n",
       "4  Positive  Positive  \n",
       "5  Positive  Negative  \n",
       "6  Positive  Negative  \n",
       "7  Positive  Positive  \n",
       "8   Neutral   Neutral  \n",
       "9  Positive  Positive  "
      ]
     },
     "execution_count": 738,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweetdf.iloc[0:10][['cleanedText','afinn', 'hlsent']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 739,
   "metadata": {},
   "outputs": [],
   "source": [
    "#takes a little longer - needs to check multiple lists\n",
    "tweetdf['hlsent'] = tweetdf.cleanedText.apply(lambda x: hl_sent(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 740,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cleanedText</th>\n",
       "      <th>afinn</th>\n",
       "      <th>hlsent</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>rt lady 45 kamala laughing stupidity built president</td>\n",
       "      <td>Positive</td>\n",
       "      <td>Negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>rt trump authority declare national emergency secure border congress gave president</td>\n",
       "      <td>Positive</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>alternate reality john bussey live keeps insisting americans</td>\n",
       "      <td>Neutral</td>\n",
       "      <td>Neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>rt dems want feel guilty demanding guilt lay illegals come amp</td>\n",
       "      <td>Negative</td>\n",
       "      <td>Negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>rt trump money schools housing gyms saying important</td>\n",
       "      <td>Positive</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>rt lady 45 kamala laughing stupidity built president</td>\n",
       "      <td>Positive</td>\n",
       "      <td>Negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>want wants issue wall built think tha</td>\n",
       "      <td>Positive</td>\n",
       "      <td>Negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>rt trump money schools housing gyms saying important</td>\n",
       "      <td>Positive</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>rt la révolution est féministe come</td>\n",
       "      <td>Neutral</td>\n",
       "      <td>Neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>rt trump money schools housing gyms saying important</td>\n",
       "      <td>Positive</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                           cleanedText  \\\n",
       "0                                 rt lady 45 kamala laughing stupidity built president   \n",
       "1  rt trump authority declare national emergency secure border congress gave president   \n",
       "2                         alternate reality john bussey live keeps insisting americans   \n",
       "3                       rt dems want feel guilty demanding guilt lay illegals come amp   \n",
       "4                                 rt trump money schools housing gyms saying important   \n",
       "5                                 rt lady 45 kamala laughing stupidity built president   \n",
       "6                                                want wants issue wall built think tha   \n",
       "7                                 rt trump money schools housing gyms saying important   \n",
       "8                                                  rt la révolution est féministe come   \n",
       "9                                 rt trump money schools housing gyms saying important   \n",
       "\n",
       "      afinn    hlsent  \n",
       "0  Positive  Negative  \n",
       "1  Positive  Positive  \n",
       "2   Neutral   Neutral  \n",
       "3  Negative  Negative  \n",
       "4  Positive  Positive  \n",
       "5  Positive  Negative  \n",
       "6  Positive  Negative  \n",
       "7  Positive  Positive  \n",
       "8   Neutral   Neutral  \n",
       "9  Positive  Positive  "
      ]
     },
     "execution_count": 740,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweetdf.iloc[0:10][['cleanedText','afinn', 'hlsent']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Again, checked sentiments on both the data frames and results are same for first 10 rows, looks like cleaning wasn't needed ...just kidding.. may be helpful for modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 741,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(11788, 8)\n"
     ]
    }
   ],
   "source": [
    "#new GI dictionary\n",
    "import re\n",
    "\n",
    "GI_frame = pd.read_csv('C:/Users/email/Downloads/' +'inquirerbasic.csv',\n",
    "                        usecols=[0,1,2,3,8,10,184,185], na_values = [\" \"])\n",
    "\n",
    "GI_frame.fillna('', inplace=True)\n",
    "print(GI_frame.shape)\n",
    "\n",
    "# get rid of multiple counts\n",
    "GI_frame['NewEntry'] = GI_frame['Entry'].str.extract('([A-Z]\\w{0,})', expand = False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 742,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Entry</th>\n",
       "      <th>Source</th>\n",
       "      <th>Positiv</th>\n",
       "      <th>Negativ</th>\n",
       "      <th>Strong</th>\n",
       "      <th>Weak</th>\n",
       "      <th>Othtags</th>\n",
       "      <th>Defined</th>\n",
       "      <th>NewEntry</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>A</td>\n",
       "      <td>H4Lvd</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>DET ART</td>\n",
       "      <td>| article: Indefinite singular article--some or any one</td>\n",
       "      <td>A</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ABANDON</td>\n",
       "      <td>H4Lvd</td>\n",
       "      <td></td>\n",
       "      <td>Negativ</td>\n",
       "      <td></td>\n",
       "      <td>Weak</td>\n",
       "      <td>SUPV</td>\n",
       "      <td>|</td>\n",
       "      <td>ABANDON</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ABANDONMENT</td>\n",
       "      <td>H4</td>\n",
       "      <td></td>\n",
       "      <td>Negativ</td>\n",
       "      <td></td>\n",
       "      <td>Weak</td>\n",
       "      <td>Noun</td>\n",
       "      <td>|</td>\n",
       "      <td>ABANDONMENT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ABATE</td>\n",
       "      <td>H4Lvd</td>\n",
       "      <td></td>\n",
       "      <td>Negativ</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>SUPV</td>\n",
       "      <td>|</td>\n",
       "      <td>ABATE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ABATEMENT</td>\n",
       "      <td>Lvd</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>Noun</td>\n",
       "      <td></td>\n",
       "      <td>ABATEMENT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>ABDICATE</td>\n",
       "      <td>H4</td>\n",
       "      <td></td>\n",
       "      <td>Negativ</td>\n",
       "      <td></td>\n",
       "      <td>Weak</td>\n",
       "      <td>SUPV</td>\n",
       "      <td>|</td>\n",
       "      <td>ABDICATE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>ABHOR</td>\n",
       "      <td>H4</td>\n",
       "      <td></td>\n",
       "      <td>Negativ</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>SUPV</td>\n",
       "      <td>|</td>\n",
       "      <td>ABHOR</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>ABIDE</td>\n",
       "      <td>H4</td>\n",
       "      <td>Positiv</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>SUPV</td>\n",
       "      <td>|</td>\n",
       "      <td>ABIDE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>ABILITY</td>\n",
       "      <td>H4Lvd</td>\n",
       "      <td>Positiv</td>\n",
       "      <td></td>\n",
       "      <td>Strong</td>\n",
       "      <td></td>\n",
       "      <td>Noun</td>\n",
       "      <td></td>\n",
       "      <td>ABILITY</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>ABJECT</td>\n",
       "      <td>H4</td>\n",
       "      <td></td>\n",
       "      <td>Negativ</td>\n",
       "      <td></td>\n",
       "      <td>Weak</td>\n",
       "      <td>Modif</td>\n",
       "      <td>|</td>\n",
       "      <td>ABJECT</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         Entry Source  Positiv  Negativ  Strong  Weak  Othtags  \\\n",
       "0            A  H4Lvd                                  DET ART   \n",
       "1      ABANDON  H4Lvd           Negativ          Weak     SUPV   \n",
       "2  ABANDONMENT     H4           Negativ          Weak     Noun   \n",
       "3        ABATE  H4Lvd           Negativ                   SUPV   \n",
       "4    ABATEMENT    Lvd                                     Noun   \n",
       "5     ABDICATE     H4           Negativ          Weak     SUPV   \n",
       "6        ABHOR     H4           Negativ                   SUPV   \n",
       "7        ABIDE     H4  Positiv                            SUPV   \n",
       "8      ABILITY  H4Lvd  Positiv           Strong           Noun   \n",
       "9       ABJECT     H4           Negativ          Weak    Modif   \n",
       "\n",
       "                                                   Defined     NewEntry  \n",
       "0  | article: Indefinite singular article--some or any one            A  \n",
       "1                                                        |      ABANDON  \n",
       "2                                                        |  ABANDONMENT  \n",
       "3                                                        |        ABATE  \n",
       "4                                                             ABATEMENT  \n",
       "5                                                        |     ABDICATE  \n",
       "6                                                        |        ABHOR  \n",
       "7                                                        |        ABIDE  \n",
       "8                                                               ABILITY  \n",
       "9                                                        |       ABJECT  "
      ]
     },
     "execution_count": 742,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# let's see first 10 tweets\n",
    "GI_frame.head(10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 743,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GI dictionary size: 11788 words of which 8559 are unique.\n",
      "Positive words: 1915\n",
      "Negative words: 2291\n",
      "Strong words: 1902\n",
      "Weak words: 755\n"
     ]
    }
   ],
   "source": [
    "#let's create lists: pos, neg, strong, weak\n",
    "\n",
    "GIlist = GI_frame['NewEntry'].tolist()\n",
    "GIlist = list(map(lambda x: str(x).lower(), GIlist))\n",
    "GIset = set(GIlist)\n",
    "print(\"GI dictionary size: \" + str(len(GIlist)) + \" words of which \" +str(len(GIset))+ \" are unique.\")\n",
    "\n",
    "\n",
    "GIpos = GI_frame['NewEntry'][GI_frame['Positiv'].str.contains('Positiv')].tolist()\n",
    "GIneg = GI_frame['NewEntry'][GI_frame['Negativ'].str.contains('Negativ')].tolist()\n",
    "GIstrong = GI_frame['NewEntry'][GI_frame['Strong'].str.contains('Strong')].tolist()\n",
    "GIweak = GI_frame['NewEntry'][GI_frame['Weak'].str.contains('Weak')].tolist()\n",
    "GIpos = list(map(lambda x: str(x).lower(), GIpos))\n",
    "GIneg = list(map(lambda x: str(x).lower(), GIneg))\n",
    "GIstrong = list(map(lambda x: str(x).lower(), GIstrong))\n",
    "GIweak = list(map(lambda x: str(x).lower(), GIweak))\n",
    "\n",
    "print(\"Positive words: \" + str(len(GIpos)))\n",
    "print(\"Negative words: \" + str(len(GIneg)))\n",
    "print(\"Strong words: \"+ str(len(GIstrong)))\n",
    "print(\"Weak words: \" + str(len(GIweak)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 744,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'pass', 'even', 'mind', 'make', 'particular', 'fine', 'board', 'hand', 'laugh', 'hit', 'deal', 'help', 'arrest', 'fun', 'order', 'matter'}\n",
      "~~~~~~\n",
      "{'scared', 'broke', 'lower', 'patient', 'limit', 'support', 'control', 'upset', 'bound', 'founder', 'shock', 'break', 'order', 'restrict', 'long', 'fear', 'wound', 'stick', 'split', 'look', 'convict', 'pass', 'run', 'whip', 'divide', 'excuse', 'occasion', 'beat', 'scare', 'ruin', 'press', 'hard', 'few', 'blind', 'surround', 'shift', 'can'}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Decisions will need to be made on how to deal with these\n",
    "\n",
    "print(set(GIpos).intersection(set(GIneg)))\n",
    "print(\"~~~~~~\")\n",
    "print (set(GIstrong).intersection(set(GIweak)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 745,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(11788, 10)\n"
     ]
    }
   ],
   "source": [
    "#Let's take the most common use based on info in the \"defined\" column\n",
    "# We could also use the POS here\n",
    "\n",
    "#define function to pull out percent\n",
    "def get_digits(text):\n",
    "    temp_num = ''.join(list(filter(str.isdigit, text)))\n",
    "    if temp_num == '':\n",
    "        temp_num = 100\n",
    "    return temp_num\n",
    "\n",
    "#create new column for digits - not strictly necessary since it's not relevant for all entries but fastest way\n",
    "GI_frame['Percent'] = GI_frame['Defined'].map(lambda x: get_digits(x))\n",
    "\n",
    "print(GI_frame.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 746,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         Entry Source  Positiv  Negativ  Strong  Weak  Othtags  \\\n",
      "0            A  H4Lvd                                  DET ART   \n",
      "1      ABANDON  H4Lvd           Negativ          Weak     SUPV   \n",
      "2  ABANDONMENT     H4           Negativ          Weak     Noun   \n",
      "3        ABATE  H4Lvd           Negativ                   SUPV   \n",
      "4    ABATEMENT    Lvd                                     Noun   \n",
      "5     ABDICATE     H4           Negativ          Weak     SUPV   \n",
      "6        ABHOR     H4           Negativ                   SUPV   \n",
      "7        ABIDE     H4  Positiv                            SUPV   \n",
      "8      ABILITY  H4Lvd  Positiv           Strong           Noun   \n",
      "9       ABJECT     H4           Negativ          Weak    Modif   \n",
      "\n",
      "                                                   Defined     NewEntry  \\\n",
      "0  | article: Indefinite singular article--some or any one            A   \n",
      "1                                                        |      ABANDON   \n",
      "2                                                        |  ABANDONMENT   \n",
      "3                                                        |        ABATE   \n",
      "4                                                             ABATEMENT   \n",
      "5                                                        |     ABDICATE   \n",
      "6                                                        |        ABHOR   \n",
      "7                                                        |        ABIDE   \n",
      "8                                                               ABILITY   \n",
      "9                                                        |       ABJECT   \n",
      "\n",
      "  Percent  \n",
      "0     100  \n",
      "1     100  \n",
      "2     100  \n",
      "3     100  \n",
      "4     100  \n",
      "5     100  \n",
      "6     100  \n",
      "7     100  \n",
      "8     100  \n",
      "9     100  \n"
     ]
    }
   ],
   "source": [
    "print(GI_frame.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 747,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(9085, 10)\n"
     ]
    }
   ],
   "source": [
    "#pull out only the versions of terms used more than 50% of the time\n",
    "GI_frame.Percent.astype(int) > 50\n",
    "GI_frame = GI_frame[GI_frame.Percent.astype(int) > 50]\n",
    "print(GI_frame.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 748,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GI dictionary size: 9085 words of which 8439 are unique.\n",
      "Positive words: 1597\n",
      "Negative words: 1971\n",
      "Strong words: 1418\n",
      "Weak words: 611\n"
     ]
    }
   ],
   "source": [
    "GIlist = GI_frame['NewEntry'].tolist()\n",
    "GIlist = list(map(lambda x: str(x).lower(), GIlist))\n",
    "GIset = set(GIlist)\n",
    "print(\"GI dictionary size: \" + str(len(GIlist)) + \" words of which \" +str(len(GIset))+ \" are unique.\")\n",
    "\n",
    "\n",
    "GIpos = GI_frame['NewEntry'][GI_frame['Positiv'].str.contains('Positiv')].tolist()\n",
    "GIneg = GI_frame['NewEntry'][GI_frame['Negativ'].str.contains('Negativ')].tolist()\n",
    "GIstrong = GI_frame['NewEntry'][GI_frame['Strong'].str.contains('Strong')].tolist()\n",
    "GIweak = GI_frame['NewEntry'][GI_frame['Weak'].str.contains('Weak')].tolist()\n",
    "GIpos = list(map(lambda x: str(x).lower(), GIpos))\n",
    "GIneg = list(map(lambda x: str(x).lower(), GIneg))\n",
    "GIstrong = list(map(lambda x: str(x).lower(), GIstrong))\n",
    "GIweak = list(map(lambda x: str(x).lower(), GIweak))\n",
    "\n",
    "print(\"Positive words: \" + str(len(GIpos)))\n",
    "print(\"Negative words: \" + str(len(GIneg)))\n",
    "print(\"Strong words: \"+ str(len(GIstrong)))\n",
    "print(\"Weak words: \" + str(len(GIweak)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 749,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "set()\n",
      "~~~~~~\n",
      "{'founder', 'excuse', 'split', 'support', 'convict', 'can'}\n"
     ]
    }
   ],
   "source": [
    "# We fixed the orientation duplication but didn't deal with the strength clashes.  \n",
    "\n",
    "print(set(GIpos).intersection(set(GIneg)))\n",
    "print(\"~~~~~~\")\n",
    "print (set(GIstrong).intersection(set(GIweak)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 750,
   "metadata": {},
   "outputs": [],
   "source": [
    "#let's create index\n",
    "\n",
    "\n",
    "def gi_sent(inputstring, show = None):\n",
    "\n",
    "    poscount = 0\n",
    "    negcount = 0\n",
    "    i = 0\n",
    "\n",
    "\n",
    "    for word in inputstring.split():\n",
    "        if i > 0:\n",
    "            prev = inputstring.split().pop(i-1)\n",
    "        else:\n",
    "            prev =\"\"\n",
    "\n",
    "        #create scalar for strong and weak words.  Strong words double, weak words add half\n",
    "        if GIstrong.count(word):\n",
    "            scale = 2\n",
    "            if show != None:\n",
    "                print(\"Strong: \" + word) \n",
    "        elif GIweak.count(word):\n",
    "            if show != None: \n",
    "                print(\"Weak: \" + word)\n",
    "            scale = 0.5\n",
    "        else:\n",
    "            scale = 1\n",
    "            \n",
    "        if GIpos.count(word):\n",
    "            if show != None:\n",
    "                print(\"Postive: \" + word ) \n",
    "            poscount +=1*scale\n",
    "        elif GIneg.count(word):\n",
    "            if show != None:\n",
    "                print(\"Negative: \" + word )\n",
    "            negcount +=1*scale\n",
    "            \n",
    "        i+=1\n",
    "    \n",
    "    if poscount+negcount > 0:\n",
    "        t = float((poscount - negcount)/(poscount+negcount))\n",
    "        \n",
    "    else:\n",
    "        t = 0\n",
    "    \n",
    "    \n",
    "    if t > 0:\n",
    "        tone = \"Positive\"\n",
    "    elif t < 0:\n",
    "        tone = \"Negative\"\n",
    "    else:\n",
    "        tone = \"Neutral\"\n",
    "    \n",
    "    return tone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 751,
   "metadata": {},
   "outputs": [],
   "source": [
    "# try GI on original tweet data frame\n",
    "tweetdf['gisent'] = tweetdf.tweetText.apply(lambda x: gi_sent(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 752,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweetText</th>\n",
       "      <th>afinn</th>\n",
       "      <th>hlsent</th>\n",
       "      <th>gisent</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>rt lady 45 kamala everyone is laughing at your stupidity the will be built and you will never be president</td>\n",
       "      <td>Positive</td>\n",
       "      <td>Negative</td>\n",
       "      <td>Negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>rt kissmybot trump has the authority to declare a national emergency to secure our border congress gave the president that author</td>\n",
       "      <td>Positive</td>\n",
       "      <td>Positive</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>in what alternate reality does john bussey live he keeps insisting that most americans are against the and t</td>\n",
       "      <td>Neutral</td>\n",
       "      <td>Neutral</td>\n",
       "      <td>Negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>rt dems want us to feel guilty for demanding a how much guilt do they lay on illegals who come here to take what s ours amp s</td>\n",
       "      <td>Negative</td>\n",
       "      <td>Negative</td>\n",
       "      <td>Negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>rt trump to take money for his from schools housing and gyms saying it didnt seem that important to him  and</td>\n",
       "      <td>Positive</td>\n",
       "      <td>Positive</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>rt lady 45 kamala everyone is laughing at your stupidity the will be built and you will never be president</td>\n",
       "      <td>Positive</td>\n",
       "      <td>Negative</td>\n",
       "      <td>Negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>does not want the he wants it as a issue if the wall was built do you think tha</td>\n",
       "      <td>Positive</td>\n",
       "      <td>Negative</td>\n",
       "      <td>Neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>rt trump to take money for his from schools housing and gyms saying it didnt seem that important to him  and</td>\n",
       "      <td>Positive</td>\n",
       "      <td>Positive</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>rt la révolution est féministe come on</td>\n",
       "      <td>Neutral</td>\n",
       "      <td>Neutral</td>\n",
       "      <td>Neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>rt trump to take money for his from schools housing and gyms saying it didnt seem that important to him  and</td>\n",
       "      <td>Positive</td>\n",
       "      <td>Positive</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                           tweetText  \\\n",
       "0                         rt lady 45 kamala everyone is laughing at your stupidity the will be built and you will never be president   \n",
       "1  rt kissmybot trump has the authority to declare a national emergency to secure our border congress gave the president that author   \n",
       "2                       in what alternate reality does john bussey live he keeps insisting that most americans are against the and t   \n",
       "3      rt dems want us to feel guilty for demanding a how much guilt do they lay on illegals who come here to take what s ours amp s   \n",
       "4                       rt trump to take money for his from schools housing and gyms saying it didnt seem that important to him  and   \n",
       "5                         rt lady 45 kamala everyone is laughing at your stupidity the will be built and you will never be president   \n",
       "6                                                    does not want the he wants it as a issue if the wall was built do you think tha   \n",
       "7                       rt trump to take money for his from schools housing and gyms saying it didnt seem that important to him  and   \n",
       "8                                                                                             rt la révolution est féministe come on   \n",
       "9                       rt trump to take money for his from schools housing and gyms saying it didnt seem that important to him  and   \n",
       "\n",
       "      afinn    hlsent    gisent  \n",
       "0  Positive  Negative  Negative  \n",
       "1  Positive  Positive  Positive  \n",
       "2   Neutral   Neutral  Negative  \n",
       "3  Negative  Negative  Negative  \n",
       "4  Positive  Positive  Positive  \n",
       "5  Positive  Negative  Negative  \n",
       "6  Positive  Negative   Neutral  \n",
       "7  Positive  Positive  Positive  \n",
       "8   Neutral   Neutral   Neutral  \n",
       "9  Positive  Positive  Positive  "
      ]
     },
     "execution_count": 752,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweetdf.iloc[0:10][['tweetText','afinn', 'hlsent', 'gisent']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 753,
   "metadata": {},
   "outputs": [],
   "source": [
    "# more complicated sentiment calculations\n",
    "\n",
    "#amplification and negation words from qdap\n",
    "negate = [\"aint\", \"arent\",\"cant\", \"couldnt\" , \"didnt\" , \"doesnt\" ,\"dont\" ,\"hasnt\" , \"isnt\" ,\"mightnt\" , \"mustnt\" ,\"neither\" ,\"never\", \"no\" ,\"nobody\" , \"nor\", \"not\" , \"shant\", \"shouldnt\", \"wasnt\" , \"werent\" ,\"wont\", \"wouldnt\"]\n",
    "amplify = [\"acute\" ,\"acutely\", \"certain\", \"certainly\" ,\"colossal\", \"colossally\",\"deep\" , \"deeply\" , \"definite\",\"definitely\" ,\"enormous\",\"enormously\" , \"extreme\", \"extremely\" ,\"great\",\"greatly\" ,\"heavily\", \"heavy\", \"high\",\"highly\" ,\"huge\",\"hugely\" , \"immense\", \"immensely\" ,\"incalculable\" ,\"incalculably\",\"massive\", \"massively\", \"more\",\"particular\" ,\"particularly\",\"purpose\", \"purposely\", \"quite\" ,\"real\" ,\"really\",\"serious\", \"seriously\", \"severe\",\"severely\" ,\"significant\" ,\"significantly\",\"sure\",\"surely\" , \"true\" ,\"truly\" ,\"vast\" , \"vastly\" , \"very\"]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 754,
   "metadata": {},
   "outputs": [],
   "source": [
    "def afinn_sent2(inputstring):\n",
    "    \n",
    "    sentcount =0\n",
    "    i=0\n",
    "    \n",
    "\n",
    "    for word in inputstring.split():\n",
    "        prev = inputstring.split().pop(i-1)\n",
    "\n",
    "        if word in afinn:\n",
    "            if (prev == 'no'):\n",
    "                sentcount = sentcount - afinn[word] - afinn[prev]\n",
    "            elif (prev == 'not'):\n",
    "                sentcount = sentcount - afinn[word]\n",
    "            else:\n",
    "                sentcount = sentcount + afinn[word]\n",
    "            i+=1\n",
    "    \n",
    "    if (sentcount < 0):\n",
    "        sentiment = 'Negative'\n",
    "    elif (sentcount >0):\n",
    "        sentiment = 'Positive'\n",
    "    else:\n",
    "        sentiment = 'Neutral'\n",
    "    \n",
    "    \n",
    "    return sentiment\n",
    "\n",
    "def hl_sent2(inputstring):\n",
    "\n",
    "    poscount = 0\n",
    "    negcount = 0\n",
    "    i = 0\n",
    "\n",
    "\n",
    "    for word in inputstring.split():\n",
    "        if i > 0:\n",
    "            prev = inputstring.split().pop(i-1)\n",
    "        else:\n",
    "            prev =\"\"\n",
    "\n",
    "        if HLpos.count(word):\n",
    "            if negate.count(prev):\n",
    "                negcount += 1\n",
    "            elif amplify.count(prev):\n",
    "                poscount +=2\n",
    "            else: \n",
    "                poscount +=1\n",
    "        elif HLneg.count(word):\n",
    "            if negate.count(prev):\n",
    "                poscount += 1\n",
    "            elif amplify.count(prev):\n",
    "                negcount +=2\n",
    "            else:\n",
    "                negcount +=1\n",
    "        i+=1\n",
    "    \n",
    "    if poscount+negcount > 0:\n",
    "        t = float((poscount - negcount)/(poscount+negcount))\n",
    "        \n",
    "    else:\n",
    "        t = 0\n",
    "    \n",
    "    \n",
    "    if t > 0:\n",
    "        tone = \"Positive\"\n",
    "    elif t < 0:\n",
    "        tone = \"Negative\"\n",
    "    else:\n",
    "        tone = \"Neutral\"\n",
    "    \n",
    "    return tone\n",
    "\n",
    "#let's create lists: pos, neg, strong, weak\n",
    "\n",
    "\n",
    "def gi_sent2(inputstring):\n",
    "\n",
    "    poscount = 0\n",
    "    negcount = 0\n",
    "    i = 0\n",
    "\n",
    "\n",
    "    for word in inputstring.split():\n",
    "        if i > 0:\n",
    "            prev = inputstring.split().pop(i-1)\n",
    "        else:\n",
    "            prev =\"\"\n",
    "\n",
    "        #create scalar for strong and weak words.  Strong words double, weak words add half\n",
    "        if GIstrong.count(word):\n",
    "            scale = 2\n",
    "        elif GIweak.count(word):\n",
    "            scale = 0.5\n",
    "        else:\n",
    "            scale = 1\n",
    "            \n",
    "        if GIpos.count(word):\n",
    "            if negate.count(prev):\n",
    "                negcount += 1*scale\n",
    "            elif amplify.count(prev):\n",
    "                poscount +=2*scale\n",
    "            else: \n",
    "                poscount +=1*scale\n",
    "        elif GIneg.count(word):\n",
    "            if negate.count(prev):\n",
    "                poscount += 1*scale\n",
    "            elif amplify.count(prev):\n",
    "                negcount +=2*scale\n",
    "            else:\n",
    "                negcount +=1*scale\n",
    "            \n",
    "        i+=1\n",
    "    \n",
    "    if poscount+negcount > 0:\n",
    "        t = float((poscount - negcount)/(poscount+negcount))\n",
    "        \n",
    "    else:\n",
    "        t = 0\n",
    "    \n",
    "    \n",
    "    if t > 0:\n",
    "        tone = \"Positive\"\n",
    "    elif t < 0:\n",
    "        tone = \"Negative\"\n",
    "    else:\n",
    "        tone = \"Neutral\"\n",
    "    \n",
    "\n",
    "    #return (negcount, poscount)\n",
    "    return tone"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's see affect of afinn2 on the labels and compare with all other dictionary... "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 755,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweetdf['afinn2'] = tweetdf.tweetText.apply(lambda x: afinn_sent2(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 760,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweetText</th>\n",
       "      <th>afinn</th>\n",
       "      <th>afinn2</th>\n",
       "      <th>hlsent</th>\n",
       "      <th>gisent</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>rt lady 45 kamala everyone is laughing at your stupidity the will be built and you will never be president</td>\n",
       "      <td>Positive</td>\n",
       "      <td>Positive</td>\n",
       "      <td>Negative</td>\n",
       "      <td>Negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>rt kissmybot trump has the authority to declare a national emergency to secure our border congress gave the president that author</td>\n",
       "      <td>Positive</td>\n",
       "      <td>Positive</td>\n",
       "      <td>Positive</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>in what alternate reality does john bussey live he keeps insisting that most americans are against the and t</td>\n",
       "      <td>Neutral</td>\n",
       "      <td>Neutral</td>\n",
       "      <td>Neutral</td>\n",
       "      <td>Negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>rt dems want us to feel guilty for demanding a how much guilt do they lay on illegals who come here to take what s ours amp s</td>\n",
       "      <td>Negative</td>\n",
       "      <td>Negative</td>\n",
       "      <td>Negative</td>\n",
       "      <td>Negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>rt trump to take money for his from schools housing and gyms saying it didnt seem that important to him  and</td>\n",
       "      <td>Positive</td>\n",
       "      <td>Positive</td>\n",
       "      <td>Positive</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>rt lady 45 kamala everyone is laughing at your stupidity the will be built and you will never be president</td>\n",
       "      <td>Positive</td>\n",
       "      <td>Positive</td>\n",
       "      <td>Negative</td>\n",
       "      <td>Negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>does not want the he wants it as a issue if the wall was built do you think tha</td>\n",
       "      <td>Positive</td>\n",
       "      <td>Positive</td>\n",
       "      <td>Negative</td>\n",
       "      <td>Neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>rt trump to take money for his from schools housing and gyms saying it didnt seem that important to him  and</td>\n",
       "      <td>Positive</td>\n",
       "      <td>Positive</td>\n",
       "      <td>Positive</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>rt la révolution est féministe come on</td>\n",
       "      <td>Neutral</td>\n",
       "      <td>Neutral</td>\n",
       "      <td>Neutral</td>\n",
       "      <td>Neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>rt trump to take money for his from schools housing and gyms saying it didnt seem that important to him  and</td>\n",
       "      <td>Positive</td>\n",
       "      <td>Positive</td>\n",
       "      <td>Positive</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                           tweetText  \\\n",
       "0                         rt lady 45 kamala everyone is laughing at your stupidity the will be built and you will never be president   \n",
       "1  rt kissmybot trump has the authority to declare a national emergency to secure our border congress gave the president that author   \n",
       "2                       in what alternate reality does john bussey live he keeps insisting that most americans are against the and t   \n",
       "3      rt dems want us to feel guilty for demanding a how much guilt do they lay on illegals who come here to take what s ours amp s   \n",
       "4                       rt trump to take money for his from schools housing and gyms saying it didnt seem that important to him  and   \n",
       "5                         rt lady 45 kamala everyone is laughing at your stupidity the will be built and you will never be president   \n",
       "6                                                    does not want the he wants it as a issue if the wall was built do you think tha   \n",
       "7                       rt trump to take money for his from schools housing and gyms saying it didnt seem that important to him  and   \n",
       "8                                                                                             rt la révolution est féministe come on   \n",
       "9                       rt trump to take money for his from schools housing and gyms saying it didnt seem that important to him  and   \n",
       "\n",
       "      afinn    afinn2    hlsent    gisent  \n",
       "0  Positive  Positive  Negative  Negative  \n",
       "1  Positive  Positive  Positive  Positive  \n",
       "2   Neutral   Neutral   Neutral  Negative  \n",
       "3  Negative  Negative  Negative  Negative  \n",
       "4  Positive  Positive  Positive  Positive  \n",
       "5  Positive  Positive  Negative  Negative  \n",
       "6  Positive  Positive  Negative   Neutral  \n",
       "7  Positive  Positive  Positive  Positive  \n",
       "8   Neutral   Neutral   Neutral   Neutral  \n",
       "9  Positive  Positive  Positive  Positive  "
      ]
     },
     "execution_count": 760,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweetdf.iloc[:10][['tweetText','afinn',  'afinn2', 'hlsent','gisent']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Above, we can see that it returns a tuple representing negative/positive based on polarity and neutral ,maybe, in case of subjectivity of each tweet. Here, we only extract polarity as it indicates the sentiment as value nearer to 1 means a positive sentiment and values nearer to -1 means a negative sentiment. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 761,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cleanedText</th>\n",
       "      <th>afinn</th>\n",
       "      <th>afinn2</th>\n",
       "      <th>hlsent</th>\n",
       "      <th>gisent</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>rt lady 45 kamala laughing stupidity built president</td>\n",
       "      <td>Positive</td>\n",
       "      <td>Positive</td>\n",
       "      <td>Negative</td>\n",
       "      <td>Negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>rt trump authority declare national emergency secure border congress gave president</td>\n",
       "      <td>Positive</td>\n",
       "      <td>Positive</td>\n",
       "      <td>Positive</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>alternate reality john bussey live keeps insisting americans</td>\n",
       "      <td>Neutral</td>\n",
       "      <td>Neutral</td>\n",
       "      <td>Neutral</td>\n",
       "      <td>Negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>rt dems want feel guilty demanding guilt lay illegals come amp</td>\n",
       "      <td>Negative</td>\n",
       "      <td>Negative</td>\n",
       "      <td>Negative</td>\n",
       "      <td>Negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>rt trump money schools housing gyms saying important</td>\n",
       "      <td>Positive</td>\n",
       "      <td>Positive</td>\n",
       "      <td>Positive</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>rt lady 45 kamala laughing stupidity built president</td>\n",
       "      <td>Positive</td>\n",
       "      <td>Positive</td>\n",
       "      <td>Negative</td>\n",
       "      <td>Negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>want wants issue wall built think tha</td>\n",
       "      <td>Positive</td>\n",
       "      <td>Positive</td>\n",
       "      <td>Negative</td>\n",
       "      <td>Neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>rt trump money schools housing gyms saying important</td>\n",
       "      <td>Positive</td>\n",
       "      <td>Positive</td>\n",
       "      <td>Positive</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>rt la révolution est féministe come</td>\n",
       "      <td>Neutral</td>\n",
       "      <td>Neutral</td>\n",
       "      <td>Neutral</td>\n",
       "      <td>Neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>rt trump money schools housing gyms saying important</td>\n",
       "      <td>Positive</td>\n",
       "      <td>Positive</td>\n",
       "      <td>Positive</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                           cleanedText  \\\n",
       "0                                 rt lady 45 kamala laughing stupidity built president   \n",
       "1  rt trump authority declare national emergency secure border congress gave president   \n",
       "2                         alternate reality john bussey live keeps insisting americans   \n",
       "3                       rt dems want feel guilty demanding guilt lay illegals come amp   \n",
       "4                                 rt trump money schools housing gyms saying important   \n",
       "5                                 rt lady 45 kamala laughing stupidity built president   \n",
       "6                                                want wants issue wall built think tha   \n",
       "7                                 rt trump money schools housing gyms saying important   \n",
       "8                                                  rt la révolution est féministe come   \n",
       "9                                 rt trump money schools housing gyms saying important   \n",
       "\n",
       "      afinn    afinn2    hlsent    gisent  \n",
       "0  Positive  Positive  Negative  Negative  \n",
       "1  Positive  Positive  Positive  Positive  \n",
       "2   Neutral   Neutral   Neutral  Negative  \n",
       "3  Negative  Negative  Negative  Negative  \n",
       "4  Positive  Positive  Positive  Positive  \n",
       "5  Positive  Positive  Negative  Negative  \n",
       "6  Positive  Positive  Negative   Neutral  \n",
       "7  Positive  Positive  Positive  Positive  \n",
       "8   Neutral   Neutral   Neutral   Neutral  \n",
       "9  Positive  Positive  Positive  Positive  "
      ]
     },
     "execution_count": 761,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweetdf['afinn2'] = tweetdf.cleanedText.apply(lambda x: afinn_sent2(x))\n",
    "tweetdf.iloc[:10][['cleanedText','afinn',  'afinn2', 'hlsent','gisent']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### let's check the difference and number of places of this change"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 757,
   "metadata": {},
   "outputs": [],
   "source": [
    "diff = np.where((tweetdf['afinn'] != tweetdf['afinn2']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 758,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'tuple'>\n",
      "(array([ 78,  80, 146, 250, 255, 303, 392], dtype=int64),)\n"
     ]
    }
   ],
   "source": [
    "print(type(diff))\n",
    "print(diff)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Only 7 tweets out of 500 tweets are different! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 759,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7\n"
     ]
    }
   ],
   "source": [
    "# So how many tweets flipped labels\n",
    "print(len(diff[0]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q3.\n",
    "Answer:\n",
    "•\tI used afinn dictionary where the simplest sentiment breakdown is to categorize tweets as positive, negative, or neutral. As I see my results, a tuple representing polarity as it indicates the sentiment as value nearer to 1 means a positive sentiment and values nearer to -1 means a negative sentiment else it’s neutral. Then moved to high level sentiment analysis using hl_sent dictionary which is more profound and wider. Result based on this dictionary looked reasonable. Further, I included GI dictionary and afinn2 as we were instructed in the classroom and lastly compared them all together.\n",
    "•\tI applied all the sentiments defining dictionaries on both my original and cleaned text and found out that results are same for afinn hl_sent, GI_sent and afinn2. Labels for afinn and afinn2 are almost same, High level sentiment (hl_sent) differ with afinn, afinn2, and GI at many places. General Inquirer depicted really unique labels than all three and I’d prefer this one on personal level. \n",
    "•\tAs of now, I’m satisfied with General Inquirer sentiment labels looks more appropriate and would not like to make any changes to my text, dictionary, or sentiment measurement to turn my labels more useful. In the above result, 1st tweet looks negative and gisent says the same, 2nd one is unanimously positive, 3rd tweet does sound little negative, 4th one is unanimously negative, 5th tweet is kind of negative as per my conscience being sarcastic but all labels are positive and finally 6th tweet looked neutral to me. Hence, for now general enquirer sentiments are worth going for.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# T4 \n",
    "Since, I applied sentiment dictionaries on both my data frames and result was same hence I don’t think changing a preprocessing step in text and dictionary would make difference."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q4.\n",
    "Yes, I may make some changes to sentiment measurement because what the available sentiment analysis are doing is useful for looking at the big picture of people feelings on a topic, but obviously, misses some of the nuance of human emotion.\n",
    "We should find next level of sentiment analysis tool that can categorize tweets based on specific emotions such as anger, fear, disgust, Joy, surprise, sarcastic and sadness. Using these emotions could give sentiment analysis a deeper context for understanding the public feelings about a topic. Having said that, we can take help of custom categories that we can determine depending on what we’re analyzing. Therefore, looking forward to those Sentiment analysis tools that use machine learning and allow us to create our own categories for researching public opinion on any topic. For example, if a company sells coffee, one could create custom categories specific to an analysis of how whiskey drinkers prefer to make their drinks. Example custom categories could include: neat, on the rocks, shots, craft cocktail (old fashioned, sazerac, etc.), or highball (whiskey coke/ginger).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "citations:\n",
    "https://www.analyticsvidhya.com/blog/2018/02/the-different-methods-deal-text-data-predictive-python/\n",
    "https://towardsdatascience.com/the-real-world-as-seen-on-twitter-sentiment-analysis-part-one-5ac2d06b63fb\n",
    "https://techcrunch.com/2018/10/30/twitters-doubling-of-character-count-from-140-to-280-had-little-impact-on-length-of-tweets/\n",
    "https://www.crimsonhexagon.com/blog/what-is-sentiment-analysis/\n",
    "https://www.cs.uic.edu/~liub/FBS/sentiment-analysis.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# -----------  The End  --------------"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
